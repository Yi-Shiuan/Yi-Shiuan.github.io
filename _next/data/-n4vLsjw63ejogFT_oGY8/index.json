{"pageProps":{"posts":[{"id":1664841600,"fileName":"postgresql-insert-or-update","url":"2022/10/04/postgresql-insert-or-update","title":"Postgresql 的Merge語法","description":"在SQL Server中有一個語法是merge的語法，他可以根據特定的條件執行特定的操作，如資料存在就更新不存在就新增。 但在Postgresql中，似乎沒有merge的語法可以使用...但在程式理先去得資料在判斷是否更新或新增這樣的情境容易導致一些後遺症.... 在google後，其實有很多方式但...嘗試後只有這個方法是可行的，所以在這邊筆記一下","date":"2022-10-04T00:00:00.000Z","tags":["postgresql","database"],"published":true,"content":"\n在SQL Server中有一個語法是`merge`的語法，他可以根據特定的條件執行特定的操作，如資料存在就更新不存在就新增。\n但在Postgresql中，似乎沒有merge的語法可以使用...但在程式理先去得資料在判斷是否更新或新增這樣的情境容易導致一些後遺症....\n在google後，其實有很多方式但...嘗試後只有這個方法是可行的，所以在這邊筆記一下\n\n> postgresql的版本是13\n\n```plsql\nINSERT INTO public.table (user_id, name, email)\nVALUES (@userid, @username, @user_email)\nON CONFLICT (user_id)\n    DO UPDATE SET (name, email) = (@username, @user_email)\nWHERE members.user_id = @userid;\n```\n當table中user_id衝突時，就會自動執行update，這樣一來根SQL server中的merge語句就相同了！\n\n\n### 參考資料\n\n[postgresql insert](https://www.postgresql.org/docs/current/sql-insert.html)\n"},{"id":1664496000,"fileName":"postgresql-dapper-json-deserialize","url":"2022/09/30/postgresql-dapper-json-deserialize","title":"透過Dapper存取Postgresql Json column 自動反序列化","description":"有時候一個物件我會把它序列化後使用JSON的方式存放到postgresql中，但把它取出後卻無法透過ORM直接做資料的對應，然後自己就土炮了做法， 把資料撈出後在針對欄位去反序列化，這樣做一開始覺得好像沒什麼，但這樣的資料變多了以後發現問體頗大，所以上網找了一些解決方案， 但關鍵字嚇得不好反而找到許多更奇怪的做法後來在stackoverflow上看到了這招真心覺得很棒的方法，於是筆記下來！","tags":["database",".net"],"date":"2022-09-30T00:00:00.000Z","published":true,"content":"\n有時候一個物件我會把它序列化後使用JSON的方式存放到postgresql中，但把它取出後卻無法透過ORM直接做資料的對應，然後自己就土炮了做法，\n把資料撈出後在針對欄位去反序列化，這樣做一開始覺得好像沒什麼，但這樣的資料變多了以後發現問體頗大，所以上網找了一些解決方案，\n但關鍵字嚇得不好反而找到許多更奇怪的做法後來在stackoverflow上看到了這招真心覺得很棒的方法，於是筆記下來！\n\n當初也有嘗試過使用`CustomPropertyTypeMap`來處理，但這個對於欄位與Class屬性名稱對應不同時使用，\n當欄位資料需要做特殊的轉換時就需要Dapper的ITypeHandler來定義什麼樣的資料型別需要怎麼去做處理，\n所以建立了一個class去繼承 `SqlMapper.ITypeHandler`然後實作類似Get/Set的物件存取的方式。\n\n```cs\npublic class JsonTypeHandler : SqlMapper.ITypeHandler\n{\n    public void SetValue(IDbDataParameter parameter, object value)\n    {\n        parameter.Value = JsonSerializer.Serialize(value);\n    }\n\n    public object Parse(Type destinationType, object value)\n    {\n        return value == null ? destinationType : JsonSerializer.Deserialize(value.ToString()!, destinationType);\n    }\n}\n```\n\n做好了這樣的物件後，只要在資料庫的連線物件上註冊`當遇到xxxx`型別時，使用JsonTypeHandler做處理，這樣一來就必須要土炮把資料撈出來後\n使用callback的方式做反序列化在僵值放回原本需要的物件中，減少了許多複雜的狀態。\n\n```cs\npublic class DbContext\n{\n    private string connectionString;\n    private readonly ILogger<DbContext> logger;\n\n    public DbContext(IServiceProvider provider)\n    {\n        略...\n\n        // 先決定目標的型別，再來把相對應的處理方式填上就行了\n        SqlMapper.AddTypeHandler(typeof(Dictionary<string, string>), new JsonTypeHandler());\n    }\n\n    /// 略...\n}\n```\n\n## 參考資料\n\n[can-dapper-deserialize-json-stored-as-text](https://stackoverflow.com/questions/49888334/can-dapper-deserialize-json-stored-as-text)\n"},{"id":1658793600,"fileName":"from-dark-age-to-mondern-deployment","url":"2022/07/26/from-dark-age-to-mondern-deployment","title":"從黑暗時代到現代化的雲端部署與維運 July 26 @ DevOps","description":"從黑暗時代到現代化的雲端部署與維運 July 26 @ DevOps","tags":["aws","devops","prevision"],"date":"2022-07-26T00:00:00.000Z","published":true,"content":"\n從黑暗時代到現代化的雲端部署與維運 July 26 @ DevOps\n\n[投影片下載](https://cdn.adhome.com.tw/blogger/從黑暗時代到現代化的雲端部署與維運July-26@DevOps.pdf)\n\n### 影片\n\n[YouTube 直播回放](https://www.youtube.com/watch?v=M0Am63ehgvE&ab_channel=DevOpsTaiwan)\n\n### 參考資料\n\n[如何讓AWS EC2開機後就能上線](https://brunojan.net/posts/2020/12/09/ec2-provisioned-self-install)\n"},{"id":1654732800,"fileName":"nodemcu-esp8266-platformio-with-clion","url":"2022/06/09/nodemcu-esp8266-platformio-with-clion","title":"使用Clion搭配platformio開發esp8266","description":"最近想把家中的電器可以跟apple homekit結合在一起省去一些麻煩順便可以帶來生活更好的狀態，但apple homekit的冷氣控制一個就要3700原， 對於家中有六台冷氣的我有點小貴，所以決定自己開發這樣的產品...順便也改造一下不太好用的窗簾機器人，讓他可以直接支援apple homekit，不需要再透過`捷徑`的方式來支援homekit\n在開始之前研究了一下許多arduino與Raspberry Pi兩者之間的選擇，發現了arduino在整個生態系來說完整不少，缺點是他基本上只支援C/C++的開發... Raspberry Pi 倒是可以使用.net或是node.js等等的語言，但許多的控制器或是傳感器支援的數量較少，並且價格也比較昂貴所以最後選擇了arduino， arduino中也有許多不同的板子，在選擇的時候的非常的困擾...後來我選擇了由樂鑫開發的ESP系列的板子好處是他已經內建了wifi功能， 所以可以直接使用不需要再加上wifi模組！","date":"2022-06-09T00:00:00.000Z","tags":["iot","platformio","arduino","esp"],"published":true,"content":"\n最近想把家中的電器可以跟apple homekit結合在一起省去一些麻煩順便可以帶來生活更好的狀態，但apple homekit的冷氣控制一個就要3700原，\n對於家中有六台冷氣的我有點小貴，所以決定自己開發這樣的產品...順便也改造一下不太好用的窗簾機器人，讓他可以直接支援apple homekit，不需要再透過`捷徑`的方式來支援homekit\n\n在開始之前研究了一下許多arduino與Raspberry Pi兩者之間的選擇，發現了arduino在整個生態系來說完整不少，缺點是他基本上只支援C/C++的開發...\nRaspberry Pi 倒是可以使用.net或是node.js等等的語言，但許多的控制器或是傳感器支援的數量較少，並且價格也比較昂貴所以最後選擇了arduino，\narduino中也有許多不同的板子，在選擇的時候的非常的困擾...後來我選擇了由樂鑫開發的ESP系列的板子好處是他已經內建了wifi功能，\n所以可以直接使用不需要再加上wifi模組！\n\n### CLion Plugin設定\n\nJetBrains開發工具真的都不錯所以我開發iot時我也選用了JetBrains的工具，在研究的時候也有人使用vs code跟arduino IDE來開發，CLion的安裝過程就不在這邊說了\n\nCLion在開發arduino必要安裝的套件有兩個，一個是`Arduino Support`另一個是`PlatformIO for CLion`這兩個就是圖片中第一個與第五個plugin，其他的你可以選用來安裝。\n\n![已安裝的CLion Plugins](https://cdn.adhome.com.tw/blogger/nodemcu-esp8266-platformio-with-clion/plugins.png)\n\n### PlatformIO cli 安裝\n\n系統安裝需要有python3，在mac中已經有內建的python3，所以透過pip安裝platformio就可以或是透過Homebrew安裝也可以\n\n```shell\n# PIP install\npip3 install platformio\n# Homebrew 安裝\nbrew install platformio\n```\n> 透過python安裝時，一定要使用python 3.8以上的版本，不然會失敗\n\n### 新增專案\n\nESP8266是NodeMCU ESP-12E的版本，所以這邊選擇NodeMCU下的ESP-12E的專案類型\n\n![CLion 新增專案](https://cdn.adhome.com.tw/blogger/nodemcu-esp8266-platformio-with-clion/new-project.png)\n\n建立完成後就會看到下面這張圖片的狀態一樣，接下來只要把程式碼寫到`src/main.cpp`就可以上傳到ESP8266的板子上了！\n\n![CLion 專案初始](https://cdn.adhome.com.tw/blogger/nodemcu-esp8266-platformio-with-clion/project-init-folder.png)\n"},{"id":1654041600,"fileName":"arduino-use-platformio-upload-error","url":"2022/06/01/arduino-use-platformio-upload-error","title":"使用platformio上傳arduino時出現錯誤代碼(0107)","description":"第一次接觸arduino，很開心地寫下了一個閃爍板子上的LED程式碼後要上傳到板子，結果上傳時發生了錯誤 出現了Failed to write to target RAM (result was 0107)這樣的錯誤代碼，試了網路上很多的方式都沒有成功，最後發現了原來serial port 不正確...","date":"2022-06-01T00:00:00.000Z","tags":["iot","platformio","arduino"],"published":true,"content":"\n第一次接觸arduino，很開心地寫下了一個閃爍板子上的LED程式碼後要上傳到板子，結果上傳時發生了錯誤\n出現了Failed to write to target RAM (result was 0107)這樣的錯誤代碼，試了網路上很多的方式都沒有成功，最後發現了原來serial port\n不正確...\n\n以下是上傳時發生的錯誤訊息資料\n```text\n/usr/local/bin/platformio -c clion run --target upload -e nodemcuv2\nProcessing nodemcuv2 (platform: espressif8266; board: nodemcuv2; framework: arduino)\n\nVerbose mode can be enabled via `-v, --verbose` option\nCONFIGURATION: https://docs.platformio.org/page/boards/espressif8266/nodemcuv2.html\nPLATFORM: Espressif 8266 (3.2.0) > NodeMCU 1.0 (ESP-12E Module)\nHARDWARE: ESP8266 80MHz, 80KB RAM, 4MB Flash\nPACKAGES:\n - framework-arduinoespressif8266 @ 3.30002.0 (3.0.2)\n - tool-esptool @ 1.413.0 (4.13)\n - tool-esptoolpy @ 1.30000.201119 (3.0.0)\n - tool-mklittlefs @ 1.203.210628 (2.3)\n - tool-mkspiffs @ 1.200.0 (2.0)\n - toolchain-xtensa @ 2.100300.210717 (10.3.0)\nLDF: Library Dependency Finder -> https://bit.ly/configure-pio-ldf\nLDF Modes: Finder ~ chain, Compatibility ~ soft\nFound 35 compatible libraries\nScanning dependencies...\nNo dependencies\nBuilding in release mode\nRetrieving maximum program size .pio/build/nodemcuv2/firmware.elf\nChecking size .pio/build/nodemcuv2/firmware.elf\nAdvanced Memory Usage is available via \"PlatformIO Home > Project Inspect\"\nRAM:   [===       ]  34.2% (used 28032 bytes from 81920 bytes)\nFlash: [===       ]  25.4% (used 265729 bytes from 1044464 bytes)\nConfiguring upload protocol...\nAVAILABLE: espota, esptool\nCURRENT: upload_protocol = esptool\nLooking for upload port...\nAuto-detected: /dev/cu.usbmodem53770161961\nUploading .pio/build/nodemcuv2/firmware.bin\nesptool.py v3.0\nSerial port /dev/cu.usbmodem53770161961\nConnecting....\nChip is ESP8266EX\nFeatures: WiFi\nCrystal is 26MHz\nMAC: e8:db:84:df:34:f7\nUploading stub...\n\nA fatal error occurred: Failed to write to target RAM (result was 0107)\n*** [upload] Error 2\n [FAILED] Took 2.01 seconds\n\nProcess finished with exit code 1\n```\n\n### 確認線材\n\n因為板子上的usb介面事mirco usb的介面，這個介面很多的線材是只有充電功能而已無法做資料傳輸，所以...首先要先確定你的線材是不是可以做資料傳輸的用途，\n否則做了再多的工作可能都無法上傳成功\n\n因此我還上了pchome購買了usb type-c to mirco usb的資料傳輸線...\n\n### 驅動程式\n\n我使用的板子是nodeMCU ESP-12E(ESP-8266的板子，晶片是CH9102X)，搭配的事CLion的IDE開發工具\n\n在網路上找到的問題主要都是第一次上傳到板子沒有安裝相對應的驅動程式所以導致錯誤發生，如果發生這個狀態好解決，安裝相對應的驅動程式就可以解決了\nESP8266的晶片有兩種一種是CH9102另一種是CP2102，網路上搜尋的時候還會出現CH340相關的資料，CH340跟CH9102是相同的所以安裝的時候可以找到CH34x的驅動安裝後即可\n但...如果還是不行，可以先確定一下電腦是否有抓到板子在做下一步\n\n```shell\npio device list\n```\n以下是我的output資訊，有看到`/dev/cu.xxxxxx`的資料，就表示真的有抓到板子囉！但如果看到了這樣的輸出，還是出現了\n`A fatal error occurred: Failed to write to target RAM (result was 0107)`的錯誤，該怎麼辦？\n\n```shell\n/dev/cu.BLTH\n------------\nHardware ID: n/a\nDescription: n/a\n\n/dev/cu.Bluetooth-Incoming-Port\n-------------------------------\nHardware ID: n/a\nDescription: n/a\n\n/dev/cu.wchusbserial53770161961\n-------------------------------\nHardware ID: USB VID:PID=1A86:55D4 SER=5377016196 LOCATION=20-2\nDescription: USB Single Serial\n\n/dev/cu.usbmodem53770161961\n---------------------------\nHardware ID: USB VID:PID=1A86:55D4 SER=5377016196 LOCATION=20-2\nDescription: USB Single Serial\n```\n\n### 確認upload port的設定\n\n安裝驅動跟確認線材的問題我卡了許久，也確認了許多次數後我發現我的輸出錯誤都是在使用`/dev/cu.usbmodem53770161961`這個serial port上傳，\n然後偶爾會出現資院忙碌中的錯誤\n```shell\ncould not open port '/dev/cu.usbmodem53770161961': [Errno 16] could not open port /dev/cu.usbmodem53770161961: [Errno 16] Resource busy:.....\n```\n\n在查詢資料過程中想起了一個設定，那就是指定上傳的serial port想說隨便試試看是不是因為serial port的問題...畢竟每次都卡在同一個port上乾脆換一個試試看\n所以我在platfromio.ini這個檔案上增加了一行upload_port的設定，沒想到就出現成功的資訊了！如果你在pio device list看到兩個port這個方法可以試試看，或許有用\n\n```text\n[env:nodemcuv2]\nplatform = espressif8266\nboard = nodemcuv2\nframework = arduino\nupload_port = /dev/cu.wchusbserial53770161961\n```\n"},{"id":1653004800,"fileName":"self-hosted-elasticsearch-8.2","url":"2022/05/20/self-hosted-elasticsearch-8.2","title":"自建Elasticsearch 8.2","description":"這週在處理自建的elasticsearch相關的狀況，今天處理告一段落後來個小筆記未來遇到才不會又搞了老半天...","date":"2022-05-20T00:00:00.000Z","tags":["elk","prevision"],"published":true,"content":"\n這週在處理在azure自建的elasticsearch相關的狀況，今天處理告一段落後來個小筆記未來遇到才不會又搞了老半天...\n\n為什麼要自建elastic呢？主要因為之前使用azure marketplace建立的elastic cloud的saas服務，現在需要把維運的職責交給SRE團隊，\n在移交的過程中發現了azure marketplace所建立的saas服務竟然不能移轉owner！只能將該服務的權限做共享而且必須將訂閱升級到Golden的等級，\n代表每個月的azure帳單又要多最少百元美金的支出...未來帳號如果被停用或是變更，有可能elastic cloud上的資料會有所影響，迫於無奈下只好走向自建的路了\n\n在自建時需要選好許多azure的服務相關的infra設計這些工作可少不了...最後決定了這些infra與azure的服務\n\n#### 1. elasticsearch server + kibana server\n\n使用vmss的服務建立虛擬機器，虛擬機器的規格是使用E2s v5的規格，根據過往經驗elasticsearch伺服器在尖峰的狀態下會使用較多的記憶體，而對於cpu要求較少一些\n對於網路的頻寬要求與對於硬碟的iops都有較大的要求，所以選用了esv5等級的機器\n\n#### 2. logstash server\n\nlogstash 服務也是採用vmss來建立虛擬機器，比較不一樣的是logstash對於cpu的要求較多，他需要去解析log還要針對log做相對應的處理，所以我們選用的是B1s的規格\n然後針對他cpu與memory的使用率做scale out/in的處理\n\n在主機中我使用的是container而非直接安裝相關的服務，然後搭配過去preversion的手法來建立伺服器。\n\n#### 3. Storage\n\n存放資料的磁區選用，這個在規劃時的poc最為困擾...在azure直接使用virtual machine的服務可以掛載managed disk的磁區，但使用vmss產生的vm是無法使用managed disk的！\n但...vmss或是vm建立的硬碟會隨這vm刪除或關閉而消失，所以我們必須得用另外一組的硬碟...後來找到了另外一種解決方案，azure file的服務，\n這樣一來就可以透過SMB掛載一個永久的硬碟，未來的log就能完整地被保留下來不會因為vm被刪除而消彌。\n\n> ps. azure file目前還不知道這樣的使用情境可以支撐多少的iops寫入與讀取，有待未來實驗\n> 根據官方文件指出每秒的iops是20,000 IOPS的讀取的速度基本上可以跟SSD差不多\n\n### 內建帳號使用 Service account token\n\n在使用8.0過後的版本kibana的服務官方預設不使用帳號密碼的方式登入，需要使用service account token的方式登入連線，然後按照官方的指引點入的連結會請你呼叫建立token的api\n```sh\n# create service account token\ncurl -X POST -u elastic:changeme \"localhost:9200/_security/service/elastic/fleet-server/credential/token?pretty\"\n```\nresponse\n```json\n{\n  \"created\": true,\n  \"token\": {\n    \"name\": \"token1\",\n    \"value\": \"AAEAAWVsYXN0aWM...vZmxlZXQtc2VydmVyL3Rva2VuMTo3TFdaSDZ\"\n  }\n}\n```\n然後在kibana上使用service account token登入，但只使用這樣的方式建立的token放在kibana上使用是可以登入elasticsearch，\n但在操作indices的時候就會出現錯誤`security_exception`說你沒有權限取得indices的資料\n```yaml\n# kibana.yml config\nserver.name: kibana\nserver.host: 0.0.0.0\nelasticsearch.hosts: [ \"http://{{elasticsearch}}:{{port}}\" ]\nxpack.monitoring.ui.container.elasticsearch.enabled: true\n\nelasticsearch.serviceAccountToken: AAEAAWVs......EtHd1E\n```\n好的，後來發現其實api呼叫的參數不對，官方在錯誤輸出的console上給的網址是會呼叫fleet-server的服務，而不是給kibana使用的所以我們要把fleet-server的token\n改成kibana服務的token，這樣就可以登入並且取得indices的資料順利的啟動kibana了\n```shell\n# for create kibana service account\ncurl -X POST -u elastic:changeme \"localhost:9200/_security/service/elastic/kibana/credential/token?pretty\"\n```\n```shell\n# check permission\ncurl -H \"Authorization: Bearer AAE.......1E\" -X GET \"localhost:9200/_security/user/_has_privileges?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"cluster\": [ \"cluster:monitor/main\" ],\n  \"index\" : [\n    {\n      \"names\": [ \"logs-apm.app-default\", \"metrics-apm.app.*-default\", \"logs-apm.error-default\", \"metrics-apm.internal-default\", \"metrics-apm.profiling-default\", \"traces-apm.rum-default\", \"traces-apm-default\" ],\n      \"privileges\": [ \"auto_configure\",\"create_doc\" ]\n    },\n    {\n      \"names\": [ \"traces-apm.sampled-default\" ],\n      \"privileges\": [ \"auto_configure\",\"create_doc\",\"maintenance\",\"monitor\",\"read\" ]\n    }\n  ]\n}\n'\n```\n\n## 後記\n\n因為azure file使用`Transaction optimized`等級的價格每GB花費是1.763台幣，價格並不便宜所以只能將hot data存放在該區域中，\n在未來會放上elastic使用cluster的方式將hot data與warm data（使用hot等級）區分開來，減少帳單的開支\n\n## 參考連結\n\n[Service Accounts (Official document)](https://www.elastic.co/guide/en/elasticsearch/reference/current/service-accounts.html)\n\n[Create service account token api (official document)](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-service-token.html)\n\n[azure file iops](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-scale-targets)\n"},{"id":1650412800,"fileName":"azure-self-install","url":"2022/04/20/azure-self-install","title":"Azure 的自動裝機","description":"在之前已經有寫過AWS的裝機處理，這次改用Azure同樣的可以讓系統從scale out到上線不需要人工的處理就可以完成系統的部署與設定","date":"2022-04-20T00:00:00.000Z","tags":["devops","azure","prevision"],"published":true,"content":"\n在之前已經有寫過AWS的裝機處理，這次改用Azure同樣的可以讓系統從scale out到上線不需要人工的處理就可以完成系統的部署與設定\n\n## Azure 的VM Scale Set\n\n在AWS是稱為Auto Scaling Group，在Azure中的名稱是Virtual Machine Scale Set(以下簡稱VMSS)，這也是一個群組設定虛擬機器的規格, 伸縮的條件, 健康檢查等等的設定\n\ncustom_data的資料就是我們準備要設定開機後要執行的腳本，這個設定了以後在azure 的Portal是無法再看到的，所以如果不是使用IaC的話一定要找個地方做紀錄\nAzure的VMSS就有內建的檢查機制(extension區塊)，可以檢查VM的服務是否正確地被啟動，若沒有在時間內被測試成功的話機器會重新收回部署\n\n在terraform 有另一個設定 extension的module， `azurerm_virtual_machine_extension`設定如果要設定`healthRepairExtension`在我測試時是會失敗的，所以一定要在extension區塊中設定。\n\n```config\nresource \"azurerm_linux_virtual_machine_scale_set\" \"sample\" {\n  name                = \"${var.prefix}-vmss\"\n  resource_group_name = data.azurerm_resource_group.main.name\n  location            = data.azurerm_resource_group.main.location\n  zone_balance        = true\n  zones               = [1, 2, 3]\n  sku                 = var.machineSize\n  instances           = var.capacity.minimum\n  admin_username      = \"azureuser\"\n  custom_data         = filebase64(\"${path.module}/custom-data.sh\")\n\n  admin_ssh_key {\n    username   = \"azureuser\"\n    public_key = data.azurerm_ssh_public_key.logstash.public_key\n  }\n\n  automatic_instance_repair {\n    enabled      = true\n    grace_period = \"PT10M\"\n  }\n\n  source_image_reference {\n    publisher = \"canonical\"\n    offer     = \"0001-com-ubuntu-server-focal\"\n    sku       = \"20_04-lts-gen2\"\n    version   = \"latest\"\n  }\n\n  os_disk {\n    storage_account_type = \"Standard_LRS\"\n    caching              = \"ReadWrite\"\n    disk_size_gb         = 30\n  }\n\n  extension {\n    name                      = \"healthRepairExtension\"\n    publisher                 = \"Microsoft.ManagedServices\"\n    type                      = \"ApplicationHealthLinux\"\n    type_handler_version      = \"1.0\"\n    automatic_upgrade_enabled = true\n    settings                  = <<settings\n      {\n        \"protocol\" : \"http\",\n        \"port\" : 80,\n        \"requestPath\" : \"/\"\n      }\n    settings\n  }\n\n  network_interface {\n    name    = \"${var.prefix}-NIC\"\n    primary = true\n\n    ip_configuration {\n      name      = \"internal\"\n      primary   = true\n      subnet_id = azurerm_subnet.subnet.id\n    }\n  }\n\n  tags = {\n    env      = var.environment\n    service  = \"logstash\"\n    createby = \"brunojan\"\n    docker   = \"yes\"\n    date     = formatdate(\"YYYY/MM/DD hh:mm:ss\", timestamp())\n    version  = var.ap_version\n  }\n}\n```\n## VMSS的擴展計畫\n\n在Azure的設定呢，說真的我還沒有非常的理解整個設定，但目前看起來的設定較為麻煩...\n\n在Profile中，一定要有一組預設的設定資料，接下來才能在設定其他的擴展策略，所以我直接hard code一組default的設定，這個設定會是主要的擴展策略。\n其他的設定基本上可以依照特定的時間，或是情境來做設定\n\n在設定中的時間設定在Azure都是使用ISO-8601的設定標準來設定，這個部份對於我來說真的很不順手，也不容易理解...\n\n```config\nresource \"azurerm_monitor_autoscale_setting\" \"autoscale\" {\n  name                = \"${var.prefix}-scale-set\"\n  resource_group_name = data.azurerm_resource_group.main.name\n  location            = data.azurerm_resource_group.main.location\n  target_resource_id  = azurerm_linux_virtual_machine_scale_set.sample.id\n\n  profile {\n    name = \"default\"\n\n    capacity {\n      default = var.capacity.minimum\n      minimum = var.capacity.minimum\n      maximum = var.capacity.maximum\n    }\n\n    dynamic \"rule\" {\n      for_each = length(var.policies) > 0 ? var.policies : []\n      content {\n        metric_trigger {\n          metric_name        = rule.value.metric\n          metric_resource_id = azurerm_linux_virtual_machine_scale_set.sample.id\n          time_grain         = rule.value.grain\n          statistic          = rule.value.statistic\n          time_window        = rule.value.duration\n          time_aggregation   = rule.value.statistic\n          operator           = rule.value.operation\n          threshold          = rule.value.threshold\n          metric_namespace   = \"microsoft.compute/virtualmachinescalesets\"\n        }\n\n        scale_action {\n          direction = rule.value.action\n          type      = \"ChangeCount\"\n          value     = rule.value.count\n          cooldown  = rule.value.cooldown\n        }\n      }\n    }\n  }\n\n  dynamic \"profile\" {\n    for_each = length(var.schedules) > 0 ? var.schedules : []\n\n    content {\n      name = profile.value.name\n\n      capacity {\n        default = profile.value.minimum\n        minimum = profile.value.minimum\n        maximum = profile.value.maximum\n      }\n\n      recurrence {\n        timezone = \"Taipei Standard Time\"\n        days     = profile.value.days\n        hours    = profile.value.hours\n        minutes  = profile.value.minutes\n      }\n\n    }\n  }\n```\n\n## Terraform azurerm_virtual_machine_scale_set\n\n這個在未來的版本中已經被棄用了，所以如果有要使用terraform的記得改用`azurerm_linux_virtual_machine_scale_set`(Linux)與azurerm_windows_virtual_machine_scale_set(Windows)\n設定上基本差不多\n\n## 參考資料\n\n[ISO-8601 wiki](https://en.wikipedia.org/wiki/ISO_8601)\n\n[Terraform azurerm_virtual_machine_scale_set](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/virtual_machine_scale_set)\n"},{"id":1641168000,"fileName":"vulnerability-scanning-for-ssl-support-algorithm","url":"2022/01/03/vulnerability-scanning-for-ssl-support-algorithm","title":"弱點掃描-SSL加密演算法不安全的演算法","description":"最近公司資安檢查報告中被檢查出了一個關於SSL的加密連線的弱點，這個弱點是因為我們沒有把不安全的加密演算法關閉導致這個弱點的產生， 在公司的政策中線上的伺服器不能任意安裝任何程式，所以`IISCrypto`就只能放棄無法使用，所以我修改的部分是使用修改幾碼的方式進行， 由於我不太熟悉資安，修改後要如何驗證呢這些已知有問題的演算法是否已經正確的被關閉了呢？","date":"2022-01-03T00:00:00.000Z","tags":["vulnerability","ssl"],"published":true,"content":"\n最近公司資安檢查報告中被檢查出了一個關於SSL的加密連線的弱點，這個弱點是因為我們沒有把不安全的加密演算法關閉導致這個弱點的產生，\n在公司的政策中線上的伺服器不能任意安裝任何程式，所以`IISCrypto`就只能放棄無法使用，所以我修改的部分是使用修改幾碼的方式進行，\n由於我不太熟悉資安，修改後要如何驗證呢這些已知有問題的演算法是否已經正確的被關閉了呢？\n\n所以我又在網路上google找到幾個腳本，透過openssl與curl的搭配可以掃描我們的server支援了哪些SSL的加密方式，有從server site的檢查與client site的檢查。\n\n```sh\n# server side\nURL=\"https://yi-shiuan.github.io/\"\n\nciphers=$(openssl ciphers 'ALL:eNULL' | sed -e 's/:/ /g')\n\n for cipher in ${ciphers[@]}\n do\n echo -n Testing $cipher...\n result=$(echo -n | openssl s_client -cipher \"$cipher\" -connect $URL 2>&1)\n if [[ \"$result\" =~ \":error:\" ]] ; then\n   error=$(echo -n $result | cut -d':' -f6)\n   echo NO \\($error\\)\n else\n   if [[ \"$result\" =~ \"Cipher is ${cipher}\" || \"$result\" =~ \"Cipher    :\" ]] ; then\n     echo YES\n   else\n     echo UNKNOWN RESPONSE\n     echo $result\n   fi\n fi\n sleep 1\n done\n```\n\n```sh\n# client site的檢查\nURL=\"https://yi-shiuan.github.io/\"\n\nDELAY=1\nciphers=$(openssl ciphers 'ALL:eNULL' | sed -e 's/:/ /g')\n\nfor cipher in ${ciphers[@]}\ndo\n    printf -v pad %30s\n    printf \"Checking ${cipher:0:30} ... \"\n    curl -s -S -o /dev/null --no-progress-meter --tls-max 1.2 --ciphers $cipher $URL\n    if [ $? -eq 0 ]; then\n        echo OK\n    fi\ndone\n```\n\n### 參考資料\n\n[IISCrypto](https://www.nartac.com/Products/IISCrypto)\n\n[叡揚資訊](https://www.gss.com.tw/blog/set-https-connect-protocols-and-ciphers)\n\n[SSL Ciphers](https://curl.se/docs/ssl-ciphers.html)\n"},{"id":1639958400,"fileName":"dotnetconf-2021@study4","url":"2021/12/20/dotnetconf-2021@study4","title":"從黑暗時代到現代化的雲端部署與維運","description":"dotnetconf 2021 ＠ study4 從黑暗時代到現代化的雲端部署與維運","tags":["azure","vmss","cd","devops","prevision","study4","dotnetconf"],"date":"2021-12-20T00:00:00.000Z","published":true,"content":"\ndotnetconf 2021 ＠ study4 從黑暗時代到現代化的雲端部署與維運\n\n[投影片下載](https://cdn.adhome.com.tw/blogger/dotnetconf2021@STUDY4.pdf)\n\n## IAC 說明\n\n[IaC Repo](https://github.com/Yi-Shiuan/dotnet-conf-iac)\n\n### website 資料夾\n\nwebsite資料夾是建立各個環境的資源檔案，內容記載該服務需要產生哪些的資源項目以及各個環境上的配置，裡面的部分採用terraform撰寫\n\n- `main.tf` => 用來設定資院建立的內容與設定\n- `variable.tf` => 定義整個腳本中有哪些變數\n- `vars 資料夾` => 每一個環境上的設定值\n\n### initial-script\n\n自動安裝腳本，這裡面的`main.sh`是整個警本的進入點，每一個服務都會有一個資料夾，資料夾內會有一個`install.sh`的檔案，這是實際上application真正執行部署的腳本\n由main.sh去下載install.sh，並且執行install.sh\n\n\n"},{"id":1635552000,"fileName":"logstash-azure-event-hub-input","url":"2021/10/30/logstash-azure-event-hub-input","title":"Logstash Azure event hub input 設定","description":"最近又在規劃ELK的設定，這次比較不一樣的地方我選擇了elastic cloud以及服務部署的方式都採用PaaS的方式作為部署， 加掛volume或是在機器上安裝filebeat都是一個比較困難的事情，所以一開始考慮使用azure blob queue的方式存放log， 但後來選擇了官方有提供的input套件，Azure Event Hub來寫log在使用logstash去讀取傳送到elasticsearch上","date":"2021-10-30T00:00:00.000Z","tags":["azure","elk"],"published":true,"content":"\n最近又在規劃ELK的設定，這次比較不一樣的地方我選擇了elastic cloud以及服務部署的方式都採用PaaS的方式作為部署，\n加掛volume或是在機器上安裝filebeat都是一個比較困難的事情，所以一開始考慮使用azure blob queue的方式存放log，\n但後來選擇了官方有提供的input套件，Azure Event Hub來寫log在使用logstash去讀取傳送到elasticsearch上\n\n根據官網的文件操作後會一直出現\n`The configuration will result in overwriting offsets. Please ensure that the each Event Hub's consumer_group is using a unique storage container.`\n這樣的錯誤訊息，也採用了進階的設定去使用，但因為只有一組採用進階的方式設定有點太過複雜所以又改回原本設定，後來想我的event hub的connection string有兩組會不會因為這樣我需要多個storage container的存放空間，\n後來將其中一組刪除後即可正常運作\n\n```config\n# 會出錯的logstash pipeline config\ninput {\n    azure_event_hubs {\n        event_hub_connections => [\"Endpoint=sb://<<event hub>>.servicebus.windows.net/;SharedAccessKeyName=logsta...\",\n            \"Endpoint=sb://<<event hub>>.servicebus.windows.net/;SharedAccessKeyName=logsta...\"]\n        storage_connection => \"DefaultEndpointsProtocol=https;...\"\n        consumer_group => \"logstash\"\n        decorate_events => true\n        threads => 8\n    }\n}\nfilter {\n    json {\n        source => \"message\"\n    }\n    date {\n        match => [ \"Timestamp\", \"ISO8601\" ]\n        target => \"@timestamp\"\n    }\n    mutate {\n        rename => [\"MessageTemplate\", \"message\" ]\n        rename => [\"Level\", \"level\" ]\n        merge => { \"message\" => \"Exception\" }\n        remove_field => [\"Exception\", \"Timestamp\"]\n    }\n}\noutput {\n    elasticsearch {\n        cloud_id => \"<<Cloud id>>\"\n        cloud_auth => \"<<user>>:<<password>>\"\n        index => \"demo-%{+YYYY.w}\"\n    }\n}\n```\n\n```config\n# 最後的 logstash pipeline config\ninput {\n    azure_event_hubs {\n        event_hub_connections => [\"Endpoint=sb://<<event hub>>.servicebus.windows.net/;SharedAccessKeyName=logsta...\"]\n        storage_connection => \"DefaultEndpointsProtocol=https;...\"\n        consumer_group => \"logstash\"\n        decorate_events => true\n        threads => 8\n    }\n}\nfilter {\n    json {\n        source => \"message\"\n    }\n    date {\n        match => [ \"Timestamp\", \"ISO8601\" ]\n        target => \"@timestamp\"\n    }\n    mutate {\n        rename => [\"MessageTemplate\", \"message\" ]\n        rename => [\"Level\", \"level\" ]\n        merge => { \"message\" => \"Exception\" }\n        remove_field => [\"Exception\", \"Timestamp\"]\n    }\n}\noutput {\n    elasticsearch {\n        cloud_id => \"<<Cloud id>>\"\n        cloud_auth => \"<<user>>:<<password>>\"\n        index => \"demo-%{+YYYY.w}\"\n    }\n}\n```\n\n\n### 參考資料\n[Github - logstash-input-azure_event_hubs](https://github.com/logstash-plugins/logstash-input-azure_event_hubs)\n"},{"id":1628812800,"fileName":"azure-managed-certificate-app-service","url":"2021/08/13/azure-managed-certificate-app-service","title":"受Azure管理的免費憑證","description":"在網頁開發中，SSL憑證已經是一個不可或缺的一件事情！網路上可以找到許多免費的憑證使用，如let's encrypt、ZeroSSL 都可以幫你產生免費的憑證，唯一麻煩的事情是三個月就需要重新處理憑證問題，在雲端供應商中AWS有提供ACM微軟也有提供類似於ACM的服務， 只要你使用了這些雲端供應商就可以免費的為你產生憑證","date":"2021-08-13T00:00:00.000Z","tags":["devops","azure","ssl"],"published":true,"content":"\n在網頁開發中，SSL憑證已經是一個不可或缺的一件事情！網路上可以找到許多免費的憑證使用，如[let's encrypt](https://letsencrypt.org/zh-tw/)、\n[ZeroSSL](https://zerossl.com/)都可以幫你產生免費的憑證，唯一麻煩的事情是三個月就需要重新處理憑證問題，當然熟悉腳本處理的可以透過一些自動化的方式來處理只是需要花點時間撰寫這些腳本\n在雲端供應商中AWS有提供ACM微軟也有提供類似於ACM的服務，只要你使用了這些雲端供應商就可以免費的為你產生憑證\n\n## 設定方式\n\nAzure 坦白說UI真的不是很直覺，這也是微軟一直以來的硬傷，這個設定其實藏在我們一直看得到的地方但又不會去點他的一個按鈕\n\n再進入設定之前，必須要將自訂的domain綁在Azure app service上（或Azure functions）才能夠產生憑證，點下`Create App Service Managed Certificate`\n後只需要點選你要的Domain 就可以產生了，整個過程約3-5分鐘左右，如果有多個子網域都需要SSL的話就多點幾次\n\n![Azure 受管的 SSL](./azure-managed-ssl.png)\n\n![Azure 產生 SSL](./create-ssl.png)\n\n產生SSL憑證後需要到`TSL/SSL Setting`中，將剛才建立的SSL綁定到相對應的Domain name上就可以了\n\n![Azure 綁定 SSL](./azure-ssl-binding.png)\n\n## 使用條件與限制\n\n憑證一定位有到期日期，在微軟提供的憑證中有效期間是6個月，在六個月到期後會自動的幫你renew這個憑證直到你刪除app service或azure functions等服務，\n必須要可以自訂domain name的規格才能夠產生免費的憑證(B1等級以上)\n\n> 此免費憑證有下列限制：\n>\n>  - 不支援萬用字元憑證。\n>  - 不支援以憑證指紋作為用戶端憑證的使用方式， (移除憑證指紋的) 計畫。\n>  - 不可匯出。\n>  - App Service 環境 (ASE) 上並不支援。\n>  - 與流量管理員整合的根域不支援。\n>  - 如果是 CNAME 對應網域的憑證，則 CNAME 必須直接對應至 {app-name}.azurewebsites.net 。\n>\n> [在 Azure App Service 中新增 TLS/SSL 憑證](https://docs.microsoft.com/zh-tw/azure/app-service/configure-ssl-certificate)\n"},{"id":1617753600,"fileName":"selenium-grid-4","url":"2021/04/07/selenium-grid-4","title":"Selenium Grid 4 體驗","description":"最近在處理系統交接並且升級相關的系統發現了Selenium Grid出了第四版現正Beta時讓我躍躍欲試， 以前Selenium Grid 3.x版本的時候UI說真的不怎麼樣，做了Selenium Grid 4的時候樣式跟整個畫面的設計全改了。 整個畫面看起來舒服了不少，但相關的東西也改了不少讓我一開始做測試的時候跌了不少次","date":"2021-04-07T00:00:00.000Z","tags":["selenium"],"published":true,"content":"\n最近在處理系統交接並且升級相關的系統發現了Selenium Grid出了第四版現正Beta時讓我躍躍欲試，\n以前Selenium Grid 3.x版本的時候UI說真的不怎麼樣，做了Selenium Grid 4的時候樣式跟整個畫面的設計全改了。\n整個畫面看起來舒服了不少，但相關的東西也改了不少讓我一開始做測試的時候跌了不少次\n\n![selenium 4](./selenium-4.png)\n\n## Selenium 架構變更\n\n在[Selenium Component](https://www.selenium.dev/documentation/en/grid/grid_4/components_of_a_grid/)文件中就給了一張架構圖，\n跟過去只有HUB跟Node的架構有所差距，在整個部署與調整上擁有了更多彈性。你可以將HUB分散在多台的Server做部署，\n或是你可以使用經典模式的HUB將這些分散的服務集中在HUB中。\n\n新增的component有以下幾個，或者是你可以使用Hub來取代這些新的component\n- Router\n- Distributor\n- Session Map\n- New Session Queuer\n- Event Bus\n\n## Selenium Node Session的改變\n\n在以前Selenium 3.x的時候，我們可以去使用MAX_SESSIONS指定該instance的Session數量，所以在自動化測試的機器叢集中我都直接給10個讓每一個node都具有10個session，\n但這一次改版後即便你加了`SE_NODE_MAX_SESSIONS`的數量，但你的CPU數量不足時也無法產生更多的Session，新版本Node的Session數量取決於你設定的最大Session與CPU最小的那一個\n在官方的github中[這一段說明](https://github.com/SeleniumHQ/docker-selenium#increasing-session-concurrency-per-container)沒仔細看還真的很容易就給他忽略過去\n\n## Dynamic Grid的使用\n\n在新版本的Selenium Grid支援了Dynamic Grid，可以在每一次測試的時候才產生相對應的目標瀏覽器，而不用預先建置好這些瀏覽器的session\n\n在公司的Selenium的測試從集中，都採用docker的方式啟動這個對我來說可以減少在infrastructure的設定與管理\n\n以下分享我的設定，在config.toml的部分讓docker-node可以使用host的docker花了一下功夫\n\n```coffeescript\n# config.toml\n\n[docker]\n# Configs have a mapping between the Docker image to use and the capabilities that need to be matched to\n# start a container with the given image.\nconfigs = [\n    \"selenium/standalone-firefox:4.0.0-beta-3-prerelease-20210402\", \"{\\\"browserName\\\": \\\"firefox\\\"}\",\n    \"selenium/standalone-chrome:4.0.0-beta-3-prerelease-20210402\", \"{\\\"browserName\\\": \\\"chrome\\\"}\",\n    \"selenium/standalone-opera:4.0.0-beta-3-prerelease-20210402\", \"{\\\"browserName\\\": \\\"operablink\\\"}\",\n    \"selenium/standalone-edge:4.0.0-beta-3-prerelease-20210402\", \"{\\\"browserName\\\": \\\"msedge\\\"}\"\n    ]\n\n# URL for connecting to the docker daemon\n# host.docker.internal works for macOS and Windows.\n# Linux could use --net=host in the `docker run` instruction or 172.17.0.1 in the URI below.\n# To have Docker listening through tcp on macOS, install socat and run the following command\n# socat -4 TCP-LISTEN:2375,fork UNIX-CONNECT:/var/run/docker.sock\nurl = \"unix:///var/run/docker.sock\"\n# Docker imagee used for video recording\nvideo-image = \"selenium/video:ffmpeg-4.3.1-20210402\"\n\n# Uncomment the following section if you are running the node on a separate VM\n# Fill out the placeholders with appropriate values\n#[server]\n#host = <ip-from-node-machine>\n#port = <port-from-node-machine>\n\n[selenium official github](https://github.com/SeleniumHQ/docker-selenium#dynamic-grid-)\n```\n\n```yml\nversion: \"3\"\nservices:\n  node-docker:\n    image: selenium/node-docker:4.0.0-beta-3-prerelease-20210402\n    user: root\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ./config.toml:/opt/bin/config.toml\n    depends_on:\n      - hub\n    environment:\n      - SE_EVENT_BUS_HOST=hub\n      - SE_EVENT_BUS_PUBLISH_PORT=4442\n      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443\n      - SE_NODE_OVERRIDE_MAX_SESSIONS=true\n      - SE_NODE_MAX_SESSIONS=10\n  hub:\n    image: selenium/hub:4.0.0-beta-3-prerelease-20210402\n    user: root\n    ports:\n      - \"4442:4442\"\n      - \"4443:4443\"\n      - \"4444:4444\"\n    logging:\n      options:\n        max-size: 100m\n        max-file: \"1\"\n    restart: always\n```\n\n## 參考連結\n\n[Selenium document](https://www.selenium.dev/documentation/en/grid/grid_4/components_of_a_grid/)\n\n[Max sessions](https://github.com/SeleniumHQ/docker-selenium#increasing-session-concurrency-per-container)\n"},{"id":1617235200,"fileName":"select-number-tdd","url":"2021/04/01/select-number-tdd","title":"Select Number TDD練習","description":"之前同事去參加面試，在面試的時候有一道上機考的題目，我覺得很有趣並且是一個很好練習的一道題目，一開始我讓我們前端的新人試著寫出這道題， 沒想到所有新人都完成了，所以我想嘗試一下TDD的方式來撰寫這道題目。","date":"2021-04-01T00:00:00.000Z","tags":["tdd","react","jest"],"published":true,"content":"\n之前同事去參加面試，在面試的時候有一道上機考的題目，我覺得很有趣並且是一個很好練習的一道題目，一開始我讓我們前端的新人試著寫出這道題，\n沒想到所有新人都完成了，所以我想嘗試一下TDD的方式來撰寫這道題目。\n\n## 練習題目需求\n\n1. 每一區有1~10個數字，點擊後會變成選取狀態，再次點擊後取消選取\n1. 在畫面上共有4個區域，每個區域都有10個數字，每區選取的號碼不能重複選取(ex: 第一區選擇\"1\"則其他區數字\"1\"為不可選)\n1. 每一區都有一個重置按鈕，點擊後該區選取的數字要被清空，並且所有區可以選取胎數字\n\n## 事後檢討\n\n一開始我的commit是有循序漸進的但後面亂掉了，變成了一個commit有多個事件處理。\n第二個是在重構的時候沒有及時的重構，所以在後面開始出現了很多個重構的commit。\n\n[repo](https://github.com/Yi-Shiuan/select-numbers)\n"},{"id":1610409600,"fileName":"the-react-first-time","url":"2021/01/12/the-react-first-time","title":"React 入門的學習之路","description":"昨天面試一位React新手雖然最後因為一些原因沒有Hire她，但他問了一個問題讓我覺得很棒：可以有什麼方法可以讓我的能力加強嗎？ 在我第一次接觸React距今已經過了好久了，久到我已經忘記我是如何啟動第一個React的APP...但身為一個React開發者又是一位面試官的角色， 未來還會有更多學習React的新手不斷進入，我想好好分享一下在面試一個React的新人時我在乎哪些技能另外也能當作學習React的入門時一個學習路徑","date":"2021-01-12T00:00:00.000Z","tags":["react"],"published":true,"content":"\n昨天面試一位React新手雖然最後因為一些原因沒有Hire她，但他問了一個問題讓我覺得很棒：`可以有什麼方法可以讓我的能力加強嗎？`\n\n在我第一次接觸React距今已經過了好久了，久到我已經忘記我是如何啟動第一個React的APP...但身為一個React開發者又是一位面試官的角色，\n未來還會有更多學習React的新手不斷進入，我想好好分享一下在面試一個React的新人時我在乎哪些技能另外也能當作學習React的入門時一個學習路徑\n\n## Local state的應用\n\n在學習React的第一步，當然建議可以先從`npx create-react-app my-app`開始，當然第一步就是在畫面印出簡單的文字來當作一個進入點，\n在這之後建議可以開始做一個簡單的ToDo List的小專案，來體驗整個React的語法、JSX與Component等等的設計，Function Component與Class Component\n都要體驗一下。\n\n## Global state的使用\n\n在學會了React的local state的應用也做了一個簡單的todo list之後，我建議學習一下Redux或是其他Global state的套件來改寫一下剛才的todo list\n嘗試幾筆新增資料後，在畫面上驅動更新顯示出來\n\n在Global state我建議的是使用Redux與React Hook的Context兩種全域的狀態管理都要學習，畢竟未來進入職場的時候不知道會使用哪一個\n\n## Virtual DOM\n\n在學習Global state之後，當然建議好好惡補一下Virtual DOM這個東西，這對未來的職涯上有很大的幫助，但又偏偏許多人忽略這個東西的養成...\n\n## Fetch API\n\n當Global state上手後，建議練習一下在React的APP中呼叫一下api取得資料與送出資料的練習，畢竟在工作上有很大的概率出現的\n在呼叫API的部分我建議做兩個的練習，一個是使用fetch的方式另一個是axios的套件，然後了解一下這兩個的使用限制與優缺\n\n## Component 的設計\n\n在Global state後，嘗試一下把todo list的Component拆分成多個Component來練習，深入了解一下state與props的相異之處使用條件與限制，\n也把Global state的狀態應用在Component中，嘗試一下使用props與Global state來更新todo list的資料，然後觀察一下其中的差異與變化。\n\n## 生命週期\n\n最後我的建議是React的生命週期，觸發render的時機等等的這些生命週期，這會讓你在React的道路上可以少踩一些地雷或是少遇到一些錯誤。\n\n\n以上是我在近期面試許多React的新手與昨天面試的同學提問後，參與自身的學習經驗來給未來新入React的朋友簡單的一個路徑。\n當然學習路徑有很多但我面試時偏好也會問這類的問題，從中鑑別候選人對React的理解程度。\n\n###\n[建立全新的 React 應用程式](https://zh-hant.reactjs.org/docs/create-a-new-react-app.html)\n"},{"id":1610064000,"fileName":"elasticsearch-tuning-and-auto-operation","url":"2021/01/08/elasticsearch-tuning-and-auto-operation","title":"Elasticsearch 效能調整與自動維運","description":"在Index Management中有個index templates的頁簽，在這裡可以改變一些index的行為或是屬性， 有些index屬性對於整個ELK的查詢或是機器的影響是很巨大的，當Log量越大的時候就需要改變一些設定， 尤其是放在雲端的ELK，如果使用越大的機器消費金額就會變得很可觀，在不是賺錢的機器上還是能省則省。","date":"2021-01-08T00:00:00.000Z","tags":["elk","devops"],"published":true,"content":"\n## 針對index的一些效能調校\n\n在Index Management中有個index templates的頁簽，在這裡可以改變一些index的行為或是屬性，\n有些index屬性對於整個ELK的查詢或是機器的影響是很巨大的，當Log量越大的時候就需要改變一些設定，\n尤其是放在雲端的ELK，如果使用越大的機器消費金額就會變得很可觀，在不是賺錢的機器上還是能省則省。\n\n`index.codec`在官方的文件中有兩個選項，預設是Default採用LZ4的壓縮，另一個選項是*best_compression*，\n使用它可以得到更高的壓縮效率但會增加CPU的附載，不過減少硬碟的空間是可觀的\n\n`refresh_interval`這個在官方文件說明是index的刷新頻率，這個選項會影響到新增的log多久內可能不會被看見，\n但refresh index他需要消耗許多的CPU來處理，這個值如果越大刷新的頻率就會降低，機器的CPU usage就不會被這個吃掉，\n可以減少一些CPU的運算\n\n`number_of_replicas`是Elasticsearch會建立多少個副本，但複本越多所需要消耗的CPU與硬碟空間就會加大\n\n```json\n{\n  \"index\": {\n    \"lifecycle\": {\n      \"name\": \"production\"\n    },\n    \"codec\": \"best_compression\",\n    \"refresh_interval\": \"15s\",\n    \"number_of_replicas\": 0\n  }\n}\n```\n\n## 自動維運Index的小技巧\n\n通常架設了ELK初期會常常觀看一些硬碟空間, CPU usage, Memory usage等等的，時間久了就會忘記了...\n\n但是硬碟的空間是有限的，Application Log其實也是有時效性的，他不需要永久的存在在硬碟上(畢竟要花錢的...)所以自動刪除index就變得很重要，\n在Index Lifecycle Policies中我會設定兩個policy，一個production跟一個non-production的，production通常會留存30天以，\n而非production的一般來說在兩週到三週就沒有參考價值了，所以非production只留存14天。\n\n![ELK index lifecycle](./index-lifecycle.png)\n\n## 參考連結\n[Elasticsearch Index Module](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html)\n\n[Tune for indexing speed](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html)\n"},{"id":1609977600,"fileName":"best-practice-layout-in-nextjs","url":"2021/01/07/best-practice-layout-in-nextjs","title":"Next.js 的Layout最佳配置","description":"新專案即將完成之際，我們開始對各個頁面上的效能與一些過去被我們忽略的問題進行修正， 在這修正的過程中我發現了在每一次頁面切換時都會出現網站的Logo消失又再出現的問題， Logo是一個經常變動的圖片所以在專案中是透過GQL取得Logo的CDN位置後才做渲染的， 再深入排查後才發現原來過去我的觀念不是非常正確所以用這篇來筆記一下正確地處理做法，當作小抄避免未來再犯同樣的錯誤。","date":"2021-01-07T00:00:00.000Z","tags":["react","next.js","frontend","layout"],"published":true,"content":"\n## 前情提要\n\n新專案即將完成之際，我們開始對各個頁面上的效能與一些過去被我們忽略的問題進行修正，\n在這修正的過程中我發現了在每一次頁面切換時都會出現網站的Logo消失又再出現的問題，\nLogo是一個經常變動的圖片所以在專案中是透過GQL取得Logo的CDN位置後才做渲染的，\n再深入排查後才發現原來過去我的觀念不是非常正確所以用這篇來筆記一下正確地處理做法，當作小抄避免未來再犯同樣的錯誤。\n\n## Next.js Layout最佳配置\n\n在Next.js的Layout最佳配置應該是把Layout的component放在`_app.tsx`中，在轉換頁面時就不會再出現前情提要的相關問題了。\n\n```typescript jsx\nfunction App({Component, pageProps }: AppProps): any {\n\tconst apolloClient: any = useApollo(pageProps.initialApolloState);\n\n\treturn <ApolloProvider client={apolloClient}>\n\t\t    <Layout>\n\t\t\t    <Component {...pageProps} />\n\t\t\t</Layout>\n\t\t</ApolloProvider>\n}\n```\n\n```typescript jsx\nconst Index: NextPage<any> = (props: any): any => {\n\treturn <>\n            {/*index content*/}\n\t\t</>\n};\n```\n\n\n### 在得到結論前的一些排查點\n\n在一開始我們的`_app.tsx`與`index.tsx`或其他page都是這樣的設計，在Layout中有`Header`與`Footer`兩個component以及負責所有頁面上的版面配置，\n其中Header這個component中去取得GQL資料，但因為他是屬於React FunctionComponent的範疇，故無法使用getInitialProps這類的function\n\n```typescript jsx\nfunction App({Component, pageProps }: AppProps): any {\n\tconst apolloClient: any = useApollo(pageProps.initialApolloState);\n\n\treturn <ApolloProvider client={apolloClient}>\n\t\t\t<Component {...pageProps} />\n\t\t</ApolloProvider>\n}\n```\n\n```typescript jsx\nconst Index: NextPage<any> = (props: any): any => {\n\treturn <Layout>\n            {/*index content*/}\n\t\t</Layout>\n};\n```\n\n1. 在方案一的我是透過React Memo的方式Cache Layout起來下次使用Layout 這個component時就不會再重新渲染，但結果是仍然每次重新渲染Layout\n1. 在_app.tsx中取得相關的Layout配置所需要的圖檔與文字，透過props的方式傳入給Layout中並讓Header在Server side 渲染，\n但因為progress image的使用所以Logo仍然會有閃一下的情況\n\n然後在官網上看到了這麼一段....\n最後我把Layout放置到`_app.tsx`中，就可以如我們預期的一開始出現了progress image的Logo再出現真正的Logo，在每次轉頁時也沒有重新渲染相關的Layout component\n\n我想主要的幾個原因是，_app.tsx的執行時間以及在轉頁渲染的最小單位是整個Next page不是採用差異的方式重新渲染。\n\n> Next.js uses the App component to initialize pages. You can override it and control the page initialization. Which allows you to do amazing things like:\n>\n> - Persisting layout between page changes\n> - Keeping state when navigating pages\n> - Custom error handling using componentDidCatch\n> - Inject additional data into pages\n> - Add global CSS\n>\n> [Next.js Custom APP](https://nextjs.org/docs/advanced-features/custom-app)\n\n### 相關連結\n\n[Next.js Custom APP](https://nextjs.org/docs/advanced-features/custom-app)\n"},{"id":1607472000,"fileName":"ec2-provisioned-self-install","url":"2020/12/09/ec2-provisioned-self-install","title":"如何讓AWS EC2開機後就能上線","description":"在雲端服務一定會遇到的是機器的擴展(scale out)與縮編(scale in)的問題，如果一個AutoScaling Group觸發了機器的成長時肯定是無法靠手動 的方式來安裝機器，所以必須要透過全資動畫的方式進行，這時候我一開始的想法是在AutoScaling發生的時候觸法Jenkins的Job來安裝系統， 但這有個問題是我整個aws的服務都必須依賴在Jenkins上，後來同事指導了一個做法只需要透過AWS的設定就可以自動裝機了！","tags":["ec2","aws","cd","devops","prevision"],"date":"2020-12-09T00:00:00.000Z","published":true,"content":"\n## 寫在前面\n\n在雲端服務一定會遇到的是機器的擴展(scale out)與縮編(scale in)的問題，如果一個AutoScaling Group觸發了機器的成長時肯定是無法靠手動\n的方式來安裝機器，所以必須要透過全資動畫的方式進行，這時候我一開始的想法是在AutoScaling發生的時候觸法Jenkins的Job來安裝系統，\n但這有個問題是我整個aws的服務都必須依賴在Jenkins上，後來同事指導了一個做法只需要透過AWS的設定就可以自動裝機了！\n\n## User Data\n\n一直以來都沒從還沒注意過AWS在建立EC2或是在Launch template介面上的`user data`，user data中的指令AWS會在我們EC2開機的過程中為我們執行\n如此一來就可以不需要依賴任何一個工具就可以完成茲動畫的作業了。\n\n![create instance 的 user data](./ec2-create-instance.png)\n\n![launch template 的 user data](./launch-template.png)\n\n```sh\n#!/bin/bash -xe\naws s3 cp s3://{{your s3 bucket}}/main.sh main.sh --region {{your region}}\nbash main.sh\n```\n\n> 在我同事的指點中，他建議在user data中不要放置帶多的指令碼而是用來下載入口指令碼與執行入口指令碼的內容就好\n\n### main.sh\n\n在入口腳本中一個很重要的事情是辨識機器需要安裝哪些東西以及要做哪些事情，但不太想讓user data有太多的版本避免團隊成員中複製時出錯，\n所以在EC2的Tag中做了一些手腳，依照EC2 Tag的設定安裝不同的軟體\n\n在取得EC2 Tag時其實需要先做很多事情，首先要先取得EC2 instance Id...但取的EC2 Instance Id前要先取得Region....\n然後發現有一個神秘的API endpoint `http://169.254.169.254/latest/dynamic/instance-identity/document` 可以取得這類的訊息，然侯回傳的是一個Json的資料格式\n\n在shell 操作Json的資料格式，不外乎就是jq這個套件了...所以我的入口腳本第一件事就是安裝jq，接下來才是去取得EC2 instance的資料\n\n當有了instance id與region時就可以取得Tag資訊了\n\n> EC2的Tag\n> 1. Service: 設定機器主要承載的服務類型\n> 2. Docker: 是否為這台機器安裝docker\n\n```sh\n#!/bin/bash -xe\n\nexec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1\nsudo yum install -y jq\nsudo apt-get install -y jq\n\nREGION=`curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | jq .region -r`\naws s3 cp s3://ec2-initial-script/functions.sh ./functions.sh --region ap-northeast-1\nsource ./functions.sh\nlog \"Start setup script\"\n\nlog \"EC2 Instance Process\" \"Region:\" $REGION\nEC2_INSTANCE_ID=`curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | jq .instanceId -r`\nlog \"EC2 instance Id: ${EC2_INSTANCE_ID}\"\nTAGS=\"$(aws ec2 describe-tags --filter \"Name=resource-id,Values=${EC2_INSTANCE_ID}\" --region ${REGION})\"\n\nlog \"DOCKER Process\"\nDOCKER=`echo $TAGS | jq -r '.Tag[] | select(.Key == \"Docker\").Value'`\nlog \"DOCKER Install ${DOCKER}\"\n\nif [[ \"$DOCKER\" == \"yes\" ]]; then\n    log \"DOCKER Install ... \"\n    sudo yum update -y\n    sudo amazon-linux-extras install docker -y\n    sudo service docker start\n    sudo usermod -a -G docker ec2-user\n    sudo systemctl restart docker\n    log \"DOCKER Install Successfully\"\n\n    log \"DOCKER-COMPOSE Install ... \"\n    sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)\" \\\n              -o /usr/local/bin/docker-compose\n    sudo chmod +x /usr/local/bin/docker-compose\n    sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n    sudo systemctl enable docker\n    log \"DOCKER-COMPOSE Install Successfully \"\nfi;\n\nSERVICE=`echo $TAGS | jq -r '.Tag[] | select(.Key == \"Service\").Value'`\nlog \"Download ${SERVICE}.sh\"\naws s3 cp s3://{{s3 bucket}}/$SERVICE/install.sh /install.sh\n\nlog \"Execute ${SERVICE} script\"\nbash install.sh \"${TAGS}\" \"${REGION}\" \"${EC2_INSTANCE_ID}\"\n\nlog \"End scripting\"\n```\n### install.sh\n\nuser data只負責下載與執行入口腳本在入口腳本中還要執行服務腳本，服務腳本是用來安裝application的！\n在入口腳本中會用EC2的Service Tag到S3 Bucket下載對應的install.sh來安裝與設定application所需要的設定\n\n#### ECS的安裝腳本\n\n```sh\n#!/bin/bash -xe\n\nsource ./functions.sh\nTAGS=$1\nREGION=$2\nINSTANCE=$3\n\nlog \"ECS Install ... \"\n\nECS_CLUSTER=`echo $TAGS | jq -r '.Tag[] | select(.Key == \"ECS:cluster\").Value'`\nsudo mkdir /etc/ecs\ncat << EOF > /etc/ecs/ecs.config\nECS_DATADIR=/data\nECS_ENABLE_TASK_IAM_ROLE=true\nECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true\nECS_LOGFILE=/log/ecs-agent.log\nECS_AVAILABLE_LOGGING_DRIVERS=[\"json-file\",\"awslogs\"]\nECS_LOGLEVEL=info\nECS_CLUSTER=$ECS_CLUSTER\nEOF\n\ndocker run -d --name ecs-agent \\\n    --detach=true \\\n    --restart=always \\\n    --volume=/var/run:/var/run \\\n    --volume=/var/log/ecs/:/log \\\n    --volume=/var/lib/ecs/data:/data \\\n    --volume=/etc/ecs:/etc/ecs \\\n    --net=host \\\n    --env-file=/etc/ecs/ecs.config \\\n    --log-driver json-file \\\n    --log-opt max-size=100m \\\n    amazon/amazon-ecs-agent:latest\n\nlog \"ECS Install Successfully \"\n```\n\n### 總結\n\n使用user data可以做到系統自動化而且還不需要依賴任何的工具，不過在使用這樣的方式時需要對整個OS, shell script（如果用windows就得對powershell熟悉）\n以及對aws cli有足夠的能力才有機會做出這樣的腳本，有了這樣的腳本才能做到100％的自動化。\n在目前已經做到完整的自動化的部署模式，我們團隊的CI server, Staging環境每天下班後自動關機上班前會自動開機並且安裝整個application\n在production的環境裡，自動化擴展時Windows的機器約莫15分鐘，Linux的機器約莫5~7分鐘就能設定完成並且服務\n\n> 在AWS的Image中只設定了預先裝好IIS的Windows機器，此目的是為了節省安裝IIS的時間成本讓機器可以更快的上線服務減少支出\n> 其他Linux的機器都只有選擇AWS Linux2的image，並沒有特別做任何設定\n>\n> windows的機器在AWS開機使用T3.Medium的速度一般來說都在15分鐘左右完成\n\n### EC2 instance 的相關時間估算\n\n| Target         |  Script | 請求機器 | 移交機器 | Provisioned | Health check |  上線時間  |\n|----------------|---------|---------|---------|-------------|--------------|-----------|\n| Windows Server | 3~5 min | 1.5 min | 7~9 min |   5 min     |     1.5min   | 15~22 min |\n| Linux Server   | 3~5 min | 1.5 min | 5~7 min |   3 min     |     1.5min   | 9~13 min  |\n\n## 相關連結\n\n[JQ github page](https://stedolan.github.io/jq/)\n"},{"id":1600473600,"fileName":"terraform-aws-infrastructure-as-code","url":"2020/09/19/terraform-aws-infrastructure-as-code","title":"Terraform 做 AWS IaC","description":"一直在公司使用ansible來做Cloud configuration但是ansible在cloud configuration上說真的略顯不足， 在之前的文章中我們很常使用`aws cli`來做相對應的處理。使用aws cli時要有更好的可讀性與維護性，通常都以JSON的格式輸入 因此在ansible中的playbook上就會有多餘的一些步驟去設定餵給aws cli的JSON。","tags":["aws","iac","terraform"],"date":"2020-09-19T00:00:00.000Z","published":true,"content":"\n## 寫在前面\n\n一直在公司使用ansible來做Cloud configuration但是ansible在cloud configuration上說真的略顯不足，\n在之前的文章中我們很常使用`aws cli`來做相對應的處理。使用aws cli時要有更好的可讀性與維護性，通常都以JSON的格式輸入\n因此在ansible中的playbook上就會有多餘的一些步驟去設定餵給aws cli的JSON。\n\n這一篇就用terraform來建立一個aws AutoScaling Group吧！\n\n## 安裝Terraform\n\nterraform的安裝其實非常簡單，在下方的參考連結中有其他的安裝方式，我這邊主要會使用mac的安裝方式\n\n在mac安裝terraform我是透過[Homebrew](https://brew.sh/index_zh-tw)來安裝terraform\n\n```shell script\n# For mac\nbrew install hashicorp/tap/terraform\n```\n\n安裝完成後做一個簡單的驗證，開啟你習慣的terminal執行以下的command就可以知道我們是不是有安裝成功了，\n如果安裝成功就會出現跟下圖一樣的資訊出來，就可以進行下一步了！\n\n\n```shell script\nterraform -help\n```\n\n![安裝terrafrom的驗證](./terraform-install.png)\n\n> 如果透過Homebrew安裝不成功，可以試試看brew upgrade，更新一下homebrew\n\n因terraform最後會產生aws cli的command，在安裝完畢後需要安裝aws cli並且設定aws的一些infomation\n並且設定aws的access key跟secert key的部分\n\n```shell script\n# install aws cli\nbrew install awscli\n\n# configure aws setting\naws configure\n```\n\n## 初始化terraform\n\n這一篇的目標是要用terrafrom建立aws 的auto scaling group，\b在達成目標前terraform前需要先做初始化\n\n初始化其實非常容易，先在你的terraform的資料夾下先建立一個`main.tf`的黨案，定義provider\n\n執行`init`後會有下方的資訊出現並且在資料夾中會有一個`.terraform`的資料夾\n\n```hlc\nterraform {\n  required_version = \">= 0.13\"\n}\n\nprovider \"aws\"\n  region = \"ap-northeast-1\"\n}\n```\n\n\n```shell script\nterraform init\n```\n\n![terraform init訊息](./terraform-init.png)\n\n## 透過terrafrom 建立 AWS AutoScaling Group\n\n接下來要建立一個`main.tf`或是使用前一步的`main.tf`，撰寫resource的設定有關於aws 的resource定義資料\n可以參考下方的參考連結中的**terraform aws provider**\n\n```hlc\nresource \"aws_autoscaling_group\" \"asg\" {\n  name                      = \"test-autoscaling-group\"\n  max_size                  = 1\n  min_size                  = 1\n  health_check_grace_period = 300\n  health_check_type         = \"ELB\"\n  desired_capacity          = 1\n  force_delete              = true\n  availability_zones        = [\"ap-northeast-1a\", \"ap-northeast-1d\"]\n  launch_template {\n    id      = \"lt-xxxxxxxx\" # 使用前需要把Id置換掉\n    version = \"$Latest\"\n  }\n}\n```\n\n### Apply\n在terraform要真的去建立資源的command 是`apply`，在真正到aws上建立資源前會有一個預覽資料等待你的確認才會真正的建立資源\n\n```shell script\nterraform apply\n```\n\n> 如果在CI的魔是可以透過auto-approve，來略過確認輸入的情況\n\n### Plan\n\n如果你想先看看資源變更的情況或是dry run時可以使用`plan`來先做預覽\n預覽後可以直接變更資源如下方圖片中的的文字 **terraform apply \"plan\"**\n\n```shell script\nterraform plan -out plan\n\n#確認後可以執行\nterraform apply \"plan\"\n```\n\n![terraform plan](./terraform-plan.png)\n\n## Oops！我的Resource被修改了！\n\n在完成第一個resource的建立後，要建立第二個autoscaling時我用了這樣的main.tf，但是出現了一些狀況...\n\n```hlc\nresource \"aws_autoscaling_group\" \"asg\" {\n  name                      = \"test-autoscaling-group2\"\n  max_size                  = 1\n  min_size                  = 1\n  health_check_grace_period = 300\n  health_check_type         = \"ELB\"\n  desired_capacity          = 1\n  force_delete              = true\n  availability_zones        = [\"ap-northeast-1a\", \"ap-northeast-1d\"]\n  launch_template {\n    id      = \"lt-xxxxxxxx\" # 使用前需要把Id置換掉\n    version = \"$Latest\"\n  }\n}\n```\n\n這時候發現剛才建立的resource被刪除並重新建立了，\n這個原因是因為你當下的資料夾出現了terraform.tfstate的檔案，將你剛才的資源資訊存放在此以便後續的資源管理\n但...我該如何產生其他新的auto scaling group呢？\n\n答案是使用terraform workspace的方式去建立一個新的workspace，讓每一個資源都是互相獨立的\n\n```shell script\nterraform workspace new auto-scaling-group-2\n```\n\n![terraform workspace](./terraform-workspace.png)\n\n接下來再重新執行一次plan指令就會發現預覽的資訊上變成了新建而不是刪除重建的狀態\n\n## Terraform的文件管理分享\n\n隨著管理的資源的建立開始會發現有許多重複的main.tf，然後要修改某個資訊要修改多個main.tf，那要如何去共用這些main.tf呢？\n\n在結構分享前我先介紹幾個terraform重要的檔案，詳細的設定請看參考連結中的`terraform configuration language`官網介紹\n\n- main.tf\n\n    main.tf是要設定AWS或是其他雲端的資源設定\n\n- variables.tf\n\n    variables.tf 則是預先定義變數，在main.tf中所用的變數資料都要在此先做定義\n\n- xxxxx.tfvars\n\n    `.tfvars` 則是預先輸入好的參數設定，後續就不需要在cli中輸入大量的資訊\n\n\n![terraform 的資料夾結構](./terraform-folder.png)\n\n在資料夾結構中，我依照aws的服務去建立相關的資料夾（如：alb, auto scaling group等等）去建立資料夾\n在每個資料夾下都會有main.tf, variables.tf, xxxxx.tfvars的檔案，在workspace的命名上會採用與tfvars的檔名相同\n並且會把workspace的名稱打在aws 服務的tag中方便未來做管理。\n\n> Tips: 在variables.tf中的變數宣告建議都放上預設值，未來要刪除資源時會更加方便！\n\n\n## 參考連結\n\n[Install Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli#install-terraform)\n\n[terraform aws provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)\n\n[terraform configuration language](https://www.terraform.io/docs/configuration/index.html)\n"},{"id":1599350400,"fileName":"jenkins-selenium-grid","url":"2020/09/06/jenkins-selenium-grid","title":"透過Jenkins啟動Selenium Grid執行自動化測試","description":"QA測試當然不是只測試RD這次上線的範圍而已，而是把過去上線的功能都要在驗過一遍才能算是經過QA測試的版本，但是隨著時間的積累線上的系統越來越多 有時候QA無法透過手動的方式完成所有驗證，這時候都會導入自動畫測試selenium來協助QA做完系統的驗證，在目前我服務的公司把CD與自動化測試的部分做了整合， 當QAT部署完成後就會驅動QA的自動化驗證，但是當自動化驗證的Job越來越多一台機器已經很難在3-5分鐘內完成自動化測試怎麼辦？","tags":["ci","selenium","jenkins"],"date":"2020-09-06T00:00:00.000Z","published":true,"content":"\n## 寫在前面\n\n一般我們上線流程都含有QA的測試階段，QA的測試與RD的開發有著很大的區別...\n\nQA測試當然不是只測試RD這次上線的範圍而已，而是把過去上線的功能都要在驗過一遍才能算是`經過QA測試`的版本，但是隨著時間的積累線上的系統越來越多\n有時候QA無法透過手動的方式完成所有驗證，這時候都會導入自動畫測試selenium來協助QA做完系統的驗證，在目前我服務的公司把CD與自動化測試的部分做了整合，\n當QAT部署完成後就會驅動QA的自動化驗證，但是當自動化驗證的Job越來越多一台機器已經很難在3-5分鐘內完成自動化測試怎麼辦？\n\n在網路上很多採用Selenium Grid的人，大多都是有一個需求是`多瀏覽器測試`，目前我們只針對chrome做測試但他也能在多台機器上為我們完成許多\n自動化測試的需求，我們則是因為許多小而美的自動化測試需要被完成\n\n> 在做自動化測試的時候，我的建議是每一次只驗證一項功能是否如我們預期的運作，然後把每個工作都拆分成小的Task來完成\n>\n> 好處是可以透過Selenium Grid同時執行驗證減少時間成本，另外也可以避免某一個功能驗證失效，而導致後面的功能無法驗證\n\n## 架構說明\n\nJenkins我是採用Master跟Agent的方式建制的，Selenium 也適用Hub跟Node的方式建置的\n\n![jenkins 與 Selenium Grid架構圖](./jenkins-selenium.png)\n\nJenkins與Selenium均採用docker並結合docker compose起動，這邊是我撰寫的`docker-compose.yml`\n\n> 在Selenium Node的部分是採用linux的版本，主要原因是雲端服務的Windows機器價格比較高，所以在部署服務上以linux為主\n\n```yaml\nversion: '3.7'\nservices:\n  master:\n    image: jenkins/jenkins:2.254\n    logging:\n      options:\n        max-size: 100m\n        max-file: \"1\"\n    restart: always\n    user: root\n    networks:\n      - net\n    ports:\n      - \"80:8080\"\n      - \"50000:50000\"\n    volumes:\n      - jenkins:/var/jenkins_home\n    environment:\n      JAVA_OPTS: -Duser.timezone=Asia/Taipei\n      JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n\n  test:\n    image: docker.boyu66.cc/common/selenium:4.2\n    user: root\n    logging:\n      options:\n        max-size: 100m\n        max-file: \"1\"\n    networks:\n      - net\n    depends_on:\n      - master\n    restart: always\n    volumes:\n      - test:/home/root\n    environment:\n      JAVA_OPTS: -Duser.timezone=Asia/Taipei\n    command: -url http://jenkins.boyu66.cc -workDir /home/root/agent {secretKey} TestAgent\n\n  hub:\n    image: selenium/hub:3.141.59-20200409\n    user: root\n    ports:\n      - \"4444:4444\"\n    logging:\n      options:\n        max-size: 100m\n        max-file: \"1\"\n    networks:\n      - net\n    depends_on:\n      - master\n    restart: always\n\n  chrome:\n    image: selenium/node-chrome:3.141.59-20200409\n    user: root\n    networks:\n      - net\n    volumes:\n      - /dev/shm:/dev/shm\n    depends_on:\n      - hub\n    environment:\n      NODE_MAX_INSTANCES: 10\n      NODE_APPLICATION_NAME: chrome\n      NODE_MAX_SESSION: 10\n      JAVA_OPTS: -Xmx512m\n\n\nnetworks:\n  net:\n    driver: bridge\n\nvolumes:\n  jenkins:\n  test:\n```\n\n## 建立Jenkins test agent的docker image\n\n在test agent 的dockerfile我選用了ubuntu作為base image，時區的部分設定到了台北並安裝openjdk等等的套件\n\n```dockerfile\nFROM ubuntu:18.04\n\nARG VERSION=4.2\nENV TZ='Asia/Taipei' \\\n    HOME=/home/root\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n\nRUN apt-get update\nRUN apt-get install -y --no-install-recommends software-properties-common \\\n      openjdk-8-jre curl git \\\n      && apt-get clean\n\nRUN curl --create-dirs -fsSLo /usr/share/jenkins/slave.jar https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/${VERSION}/remoting-${VERSION}.jar \\\n      && chmod 755 /usr/share/jenkins \\\n      && chmod 644 /usr/share/jenkins/slave.jar\n\nWORKDIR /home/root\nUSER root\n\n## Jenkins jnlp slave ##\nCOPY jenkins-slave.sh jenkins-slave.sh\n\n\nENTRYPOINT [\"bash\", \"/home/root/jenkins-slave.sh\"]\n```\n\n最後將Jenkins的Job指定到Test Agent上執行，在Selenium的部分把webdriver的設定改道遠端的位置上就可以享受Selenium Grid了！\n\n## 參考資料\n\n[Selenium Grid 中文文黨](https://wizardforcel.gitbooks.io/selenium-doc/content/official-site/selenium-grid.html)\n"},{"id":1599091200,"fileName":"ecs-deploy-preparing","url":"2020/09/03/ecs-deploy-preparing","title":"AWS ECS Preparing Release 紀錄","description":"我們在aws 做Production deploy的時候，都會有一個pre production的環境，這個環境主要是為了在部署流程結束後可以做概念性驗證的環境 （主要測試：db connection是否正常、網路連線、裝機腳本等等）另一個方面可以預熱application，不過我們開始有越來越多採用docker的 application並且使用ECS的部署模式，但是ECS的Service建立後就無法修改Target Group，因此沒辦法如同EC2的部署模式只在最後切換Target Group","tags":["ecs","aws","cd","devops","prevision","ansible"],"date":"2020-09-03T00:00:00.000Z","published":true,"content":"\n## 寫在前面\n\n我們在aws 做Production deploy的時候，都會有一個pre production的環境，這個環境主要是為了在部署流程結束後可以做概念性驗證的環境\n（主要測試：db connection是否正常、網路連線、裝機腳本等等）另一個方面可以預熱application，不過我們開始有越來越多採用docker的\napplication並且使用ECS的部署模式，但是ECS的Service建立後就無法修改Target Group，因此沒辦法如同EC2的部署模式只在最後切換Target Group\n\n## 構思&實作\n\n一開始其實想得很簡單，就是每次部署時都要產生新的 Target group 與新的ECS Service，\n將新產生的Target Group掛載到ELB的preparing規則上，然後驗證完畢後就把這組Target Group掛載到ELB的 public規則上就完成了一次的部署\n\n但...事件總是沒想像中的美好，原先預計兩週內可以完成的項目變到了三週(其實中間也有對自己的要求增加ex: script可以重跑、自動移除舊版本等等)\n\n### Deploy Configuration\n\n這個是為了減少使用者輸入資訊所做的一個設定，內容主要如下\n\n```yaml\nVPC:\nRegion:\nHealthCheck:\n    Path: \"/\"\n    UnhealthyThreshold: 2\n    HealthyThreshold: 5\nLoadBalancer:\n    Preparing:\n    Public:\nAutoScaling:\n    Desired: 2\n    MaxCount: 10\n    MinCount: 2\nPolicies:\n    - Name: Scale-By-CPU-Usage\n      CoolDown: 150\n      Threshold: 75.0\n      Metric: ECSServiceAverageCPUUtilization\n    - Name: Scale-By-Memory-Usage\n      CoolDown: 150\n      Threshold: 75.0\n      Metric: ECSServiceAverageMemoryUtilization\n```\n\n### Step1. 建立Target Group\n\n在建立Target Group其實沒什麼坑主要踩到的就是Target group的名字不能超過**32**個字所以在命名規劃上需要思考一下該如何呈現\n> 我的命名規則是：環境-服務名稱-版本號\n\n這一步執行的結果需要將內容記錄下來，在修改ELB的rule時會需要用到\n\n```yaml\n- name: Create Target Group\n  elb_target_group:\n    vpc_id: \"{{ VPC }}\"\n    region: \"{{ Region }}\"\n    state: present\n    name: \"{{ name }}\"\n    protocol: http\n    port: 80\n    health_check_protocol: http\n    health_check_interval: 30\n    health_check_timeout: 5\n    health_check_path: \"{{ HealthCheck.Path }}\"\n    unhealthy_threshold_count: \"{{ HealthCheck.UnhealthyThreshold }}\"\n    healthy_threshold_count: \"{{ HealthCheck.HealthyThreshold }}\"\n    successful_response_codes: \"200\"\n    target_type: instance\n    deregistration_delay_timeout: 60\n    tags:\n      Cluster: \"{{ cluster }}\"\n      ENV: \"{{ env }}\"\n      Version: \"{{ version }}\"\n      Name: \"{{ name }}\"\n      CreateTime: \"{{ lookup('pipe','date +%Y%m%d') }}\"\n  register: target_group_result\n```\n\n### Step2. 連結ELB 的規則\n\n這一步在ansible中這定較為複雜所以改用了aws cli來處理，如果你想用純的ansible的方式處理，可以參考\n[這篇(elb_application_lb)](https://docs.ansible.com/ansible/latest/modules/elb_application_lb_module.html#elb-application-lb-module)\n\nansible的回傳都是陣列的形式所以在存取result的時候需要取得第一筆資料（如果你建立了多個target group\n就可以用loop來處理）\n\n```yaml\n- name: Attach new target group to preparing load balancer rule\n  shell: |\n    aws elbv2 modify-rule \\\n      --actions Type=forward,TargetGroupArn={{ target_group_result.results[0].target_group_arn }} \\\n      --rule-arn {{ LoadBalancer.Preparing }} \\\n      --region {{ Region }}\n```\n\n### Step3. 註冊新的Task Definition\n\n這一步驟來說應該是最複雜的一部分，我的Task Definition是由專案中的一個Configuration檔案設定\n在CI建置完成後上傳到artifact server，在執行部屬時用ansible下載到Deploy server上在讀取到\nansible的變數中。\n\n在專案中的Configuration不是一整份的設定值，他只記錄了一部分的資訊(ex: cpu, memory, family等)\n主要的原因是有些資料需要在deploy時才能決定(ex: image version, env...)，\n所以我會在註冊task definition前先透過configuration建立一份完整的task definition，\n再透過aws cli來註冊新的task definition\n\n這邊你會有個疑問，為何不直接採用\n[ansible module](https://docs.ansible.com/ansible/latest/modules/ecs_taskdefinition_module.html#ecs-taskdefinition-module)\n呢?主要原因是之前我們就有ecs deploy的CD流程，當時候有些參數我們需要但ansible無法支援\n所以轉用aws cli的方式進行。\n\n```yaml\n- name: \"Register Task Definition\"\n  shell: |\n    aws ecs register-task-definition \\\n      --cli-input-json '{{ td_setting | to_json }}' \\\n      --region {{ Region }};\n```\n\n### Step4. 新建ECS Service\n\n這一步我依舊採用aws cli來建立service，主要的原因是我的service通常會由兩個target group\n指向兩個不同的ELB與domain，在ansible官方網站上並沒有太多的說明與所需要的參數，因此我轉用了\naws cli，如同step3我會先將所需要的設定在一個ansible task上做好 產生了`service_setting`的參數\n在執行cli時將參數轉換成JSON代入\n\n```yaml\n- name: \"Create ECS service {{ service_setting.serviceName }}\"\n  shell: |\n    aws ecs create-service \\\n      --service-name '{{ service_setting.serviceName }}' \\\n      --cli-input-json '{{ service_setting | to_json }}' \\\n      --region {{ Region }};\n  register: ecs_service_create\n```\n\n## 切換Service Version\n\n切換服務版本這是一個比較大的工程，剛才的部署的複雜度更高一些，主要概念就是將新產生的target group\n掛載到真正線上服務的ELB上，不過呢這件事情衍生了許多細細小小的項目要處理的細節也比較多\n\n### Step1. 確認Target group health count\n\n要切換前一定要先檢查Target group的target狀態，如果沒確認切換了一個還在做health check的target\ngroup就會發生線上可能當下沒有機器服務的窘境，所以第一步肯定就是確認health count\n\n```yaml\n- name: Get Target Group Informantion\n  elb_target_group_info:\n    region: \"{{ Region }}\"\n    collect_targets_health: yes\n    names: \"{{ switch_target_group }}\"\n  register: target_group_result\n  failed_when: (target_group_result | json_query('target_groups[].targets_health_description[].target_health.state') | length) < {{ AutoScaling.Desired }}\n```\n\n當如果target的health count沒有達到要求數量我就讓playbook失敗，之前在ec2的做法是要完全healthy\n才能夠往下運行但這樣會遇到當下要switch會無法switch，所以這次改變一下做法讓target的health count\n只要大於要求數量即可\n\n### Step2. 取得public當前的target group\n\n為什麼要取得當前的target group？主要原因是怕瞬斷的情況發生，所以在切換的時候會有一個時間區間同時\n有兩個版本在運行最後再將舊版本移除\n\n取得ELB中的rule資料ansible並沒有太好的方式處理，所以這道題還是只能靠aws cli來協助了，aws cli 只需要輸入public rule的arn就可以取得了\n詳細可以看[aws cli](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/elbv2/describe-rules.html) 的官方文件\n\n```yaml\n- name: \"Get load balancer rule for {{ service_name | upper }}\"\n  shell: |\n    aws elbv2 describe-rules \\\n      --rule-arns {{ LoadBalancer.Public | join(' ') }} \\\n      --region {{ Region }}\n  register: current_rules\n```\n\n接下來這個就需要比較耐心地來處理了，因為ansible shell的回傳result會有個stdout屬性，這就是aws cli最後回傳的資料，所以我們要解析這個\n資訊，然後ansible 的json_query是用[這個](https://jmespath.org/)來實作的，所以我們可以透過這個先取得我們想要的結果，當然他會有一個target group arn資料\n不過我們後面需要一些壓在target group tag上的資料，所以我在這邊把他先取回來\n\n```yaml\n- name: Get target group informantion of current load balance setting\n  elb_target_group_info:\n    region: \"{{ Region }}\"\n    target_group_arns: \"{{ current_rules.stdout | from_json | json_query('Rules[].Actions[].ForwardConfig.TargetGroups[].TargetGroupArn') }}\"\n  register: current_target_group_result\n```\n\n### Step3. 把Prepare的target group跟public的整合在一起\n\n在ansible還時沒有一個簡單的設定，去修改ELB rule的target group連結，所以還是得透過aws cli來作了\n\n>下面的ansible tasks我將它設定成一個role，主要原因是我有兩個target group在處理，\n>`set_fact`去跑loop的情況會比較複雜所以我設計成一個role，在playbook上採用loop\n>來執行這個role，如此一來邏輯比較清晰也比較好維護\n\n如此一來在public 的規則上就有兩個target group的容器在服務了，這樣的情況我會讓他維持約1分鐘，再將原先的target group移除\n\n ```yaml\n- name: Generate load balance rule setting\n  set_fact:\n    target_rule_setting: |\n      [\n        {\n          \"Type\": \"forward\",\n          \"ForwardConfig\": {\n            \"TargetGroups\": [\n              {\n                \"TargetGroupArn\": '{{ target_group[0].arn }}',\n                \"Weight\": {{ target_group[0].weight }}\n              },\n              {\n                \"TargetGroupArn\": '{{ target_group[1].arn }}',\n                \"Weight\": {{ target_group[1].weight }}\n              }\n            ],\n            \"TargetGroupStickinessConfig\": {\n              \"Enabled\": false\n            }\n          }\n        }\n      ]\n- name: \"Attach to public rule\"\n  shell: |\n    aws elbv2 modify-rule \\\n      --actions '{{ target_rule_setting | to_json }}' \\\n      --rule-arn {{ target_rule }} \\\n      --region {{ Region }}\n\n - name: \"Remove old version\"\n  shell: |\n    aws elbv2 modify-rule \\\n      --actions Type=forward,TargetGroupArn={{ item }} \\\n      --rule-arn {{ LoadBalancer.Release[ansible_loop.index0].Rule }} \\\n      --region {{ Region }}\n  ```\n\n### Step4. 處置舊版本的服務\n\n移轉成功後需要把舊版本的service關機或是移除，我的選擇是暫時關閉\n\n關閉服務聽起來很簡單，但我用aws cli執行將需求值改成0，但....一點效果都沒有！！\n\n還記得一開始設定的autoscaling嗎....沒錯就是他！當你把desired改成0，會跟autoscaling設定的發生衝突所以變得無效\n\n在調整desired改成0前，要把autoscaling設定移除才能順利的關閉service將cluster的資源釋出！\n\n```yaml\n- name: Shutdown ECS service\n  shell: |\n    aws application-autoscaling register-scalable-target \\\n      --service-namespace ecs \\\n      --scalable-dimension ecs:service:DesiredCount \\\n      --resource-id service/{{ current_target_group_result.target_groups[0].tags[\"Cluster\"] }}/{{ current_target_group_result.target_groups[0].tags[\"Name\"] }} \\\n      --min-capacity 0 \\\n      --max-capacity 0 \\\n      --region {{ Region }};\n    aws ecs update-service \\\n      --cluster {{ current_target_group_result.target_groups[0].tags[\"Cluster\"] }} \\\n      --service {{ current_target_group_result.target_groups[0].tags[\"Name\"] }} \\\n      --desired-count 0 \\\n      --region {{ Region }};\n```\n\n### Step5. 移除\n\n這是整個部署流程的尾聲了（或是你可以選擇不做....）\n\n我的做法是先將整個ecs cluster的service列出來，然後取得target group資料，再由target group資料取得tag是否含有`obsolete`的tag\n\n```yaml\n- name: Get obsolete target group info\n  elb_target_group_info:\n    region: \"{{ Region }}\"\n    target_group_arns: \"{{ target_group_arn | json_query('loadBalancers[].targetGroupArn') }}\"\n  register: ecs_tg_result\n\n- name: Delete obsolete ECS service and target group\n  shell: |\n    aws ecs delete-service \\\n      --cluster {{ ecs_tg_result.target_groups[0].tags['Cluster'] }} \\\n      --service {{ ecs_tg_result.target_groups[0].tags['Service'] }} \\\n      --region {{ Region }} ;\n    aws elbv2 delete-target-group \\\n      --target-group-arn {{ ecs_tg_result.target_groups[0].target_group_arn }} \\\n      --region {{ Region }} ;\n    aws elbv2 delete-target-group \\\n      --target-group-arn {{ ecs_tg_result.target_groups[1].target_group_arn }} \\\n      --region {{ Region }} ;\n  when: ecs_tg_result.target_groups[0].tags['Obsolete'] is defined\n```\n\n### Step6. 標記\n\n部署的最後一步，標記已經被關閉的服務變成`obsolete`\n\n這是為了下次部署可以把這些服務刪除，如果前一個步驟不做的話，這個步驟也是可以省略的。\n\n ```yaml\n- name: Configure obsolete target group\n  shell: |\n    aws elbv2 add-tags \\\n      --resource-arns {{ item.target_group_arn }} \\\n      --tags '[{\"Key\": \"Obsolete\", \"Value\": \"yes\"}]' \\\n      --region {{ Region }}\n  loop: \"{{ current_target_group_result.target_groups }}\"\n```\n"},{"id":1597881600,"fileName":"redis-data-types","url":"2020/08/20/redis-data-types","title":"Redis Data Types 介紹","description":"Redis在4.0之前其實只有5個資料型態，不過到了現在新的版本多了一些不同的形態如stream, geo等等，不過本篇只會針對原有的string, sort set, hash set, set, list這五個形態來做說明。","tags":["redis"],"date":"2020-08-20T00:00:00.000Z","published":true,"content":"\n## 前言\n\n這篇文章其實是以前在整理Redis的文件時所做的一些資料，再拿出來寫是因為現在同事剛好也需要這樣的資訊。\n\nRedis在4.0之前其實只有5個資料型態，不過到了現在新的版本多了一些不同的形態如stream, geo等等，不過本篇只會針對原有的string, sort set,\nhash set, set, list這五個形態來做說明。\n\n### String 型別\n\nString 型態可以存放 binary, string, integer, float資料，在官網的說明是最基礎的型態，單一個Key可以存放月512MB的資料。\n\n>Strings are the most basic kind of Redis value. Redis Strings are binary safe,\n>this means that a Redis string can contain any kind of data, for instance a JPEG image or a serialized Ruby object.\n>\n> [redis 官方網站](https://redis.io/topics/data-types)\n\n\n#### String 使用的幾個場景介紹\n\n1. 圖片快取 （使用binary）\n1. Configuration\n1. 累計次數、觀看累計次數\n\n> 不過根據我的使用經驗來說，資料量在1Kb內擁有較好的網路傳輸，當今天你存放的string資料大於1kb，我建議轉換成binary資料，透過Gzip做壓縮\n>才存放到Redis中才會有較好的快取效果\n\n### HashSet\n\nHash set是用來存放一組相同性質的資料，這些資料HashSet(or Dictionary)或是物件的某一屬性，與String較為不同的是他可以取回單一個欄位資料\n但String必須取回所有資料（若透過Redis module可以取得Json特定的資料），單一個Key可以存放2<sup>32</sup> - 1的資料欄位，\n\n> Redis Hashes are maps between string fields and string values, so they are the perfect data type to represent objects\n>(e.g. A User with a number of fields like name, surname, age, and so forth):\n>\n> [redis 官方網站](https://redis.io/topics/data-types)\n\n#### Hashes的使用場景\n\n1. 每次只需要取用一部分的資料\n1. C#的Dictionary或HashSet資料型態\n\n### List\n\nList 資料型態可以想像成程式語言中的Array物件或是你可以把它時作成Queue或是Stack的物件。List 單一個Key可以存放2<sup>32</sup> - 1\n\n> Redis Lists are simply lists of strings, sorted by insertion order.\nIt is possible to add elements to a Redis List pushing new elements on the head (on the left) or on the tail (on the right) of the list.\n>\n> [redis 官方網站](https://redis.io/topics/data-types)\n\n#### List的使用場景\n\n* List 物件\n* 無序資料\n* Queue 物件\n* Stack 物件\n* Array 物件\n\n### Set\n\n類似於List的資料格式但不能存入相同的資料，所以內容是不能重複的預設也不會幫你排序，Sets 單一個Key可以存放2<sup>32</sup> - 1\n\n#### Set的使用場景\n\n1. 文章中的Tag標籤\n1. 用來排除相同資料\n\n### Sorted Set\n\nSorted Set從名字就可以知道他是一個**經過**排序的Set所以當你每次有資料異動時都會自動的為你重新排序採用的是快速排序的演算法，\n這個資料結構可以說是Redis操作的時候成本最高的一個結構吧！通常的時間成本是**O(log(N))**N為Key所存放的資料總量，另外他跟Set很相似內容是不能重複的\n\n> Redis Sorted Sets are, similarly to Redis Sets, non repeating collections of Strings.\n>The difference is that every member of a Sorted Set is associated with score,\n>that is used in order to take the sorted set ordered, from the smallest to the greatest score.\n>While members are unique, scores may be repeated.\n>\n> [redis 官方網站](https://redis.io/topics/data-types)\n\n#### Sorted Set的使用場景\n\n其實也沒什麼好說的，需要有序的資料用他準沒錯\n\n### 其他\n\n其他資料還有Bitmaps, HyperLogLogs, geo, stream等等，目前我也沒有實際使用過，所以就不在此介紹了\n\n### 參考連結\n[Redis Data Types](https://redis.io/topics/data-types)\n"},{"id":1475880255,"fileName":"protobuf-serialize-and-deserialize","url":"2016/10/08/protobuf-serialize-and-deserialize","title":"[ProtoBuf] ProtoBuf Serialize 與 DeSerialize！","description":"這一篇應該是我ProtoBuf的系列文章第二篇，上一篇只簡單說了一些安裝方式與一些定義檔 的設定根本就不知道該如何使用這一個好用的序列化工具。依樣會是使用protobuf-net 這一個套件來做一個簡單的紀錄。","tags":["protobuf","serialize","deserialize"],"published":true,"date":"2016-10-07T22:44:15.000Z","content":"\n這一篇應該是我ProtoBuf的系列文章第二篇，上一篇只簡單說了一些安裝方式與一些定義檔\n的設定根本就不知道該如何使用這一個好用的序列化工具。依樣會是使用protobuf-net\n這一個套件來做一個簡單的紀錄。\n\n## ProtoBuf 序列化\n\n在使用 ProtoBuf 的第一步就必須了解 Stream 的資料格式，因為在(反)序列化\n的過程中，預設都是使用 Stream 作為媒介。序列化的部份是使用 Serializer.Serialize\n這個方法來處理。\n\n### Simple Code 1 - 序列化\n\n```cs\npublic void SerializeToStream<T>(T data, Stream stream)\n{\n    Serializer.Serialize(stream, data)\n}\n\npublic void Main()\n{\n    using (var stream = new MemoryStream())\n    {\n        this.SerializeToStream<object>(data, stream);\n    }\n}\n```\n\n主要原因是若Stream關閉後就無法做任何的操作，所以交由外部來控制Stream的資源初始與回收。\n可以在序列化成為 Array 或 String 的格式。\n\n### Simple Code 2 - Serialize to byte array and string\n\n```cs\npublic byte[] SerializeToBytes<T>(T data)\n{\n    byte[] result;\n    using (var stream = new MemoryStream())\n    {\n        this.SerializeToStream(data, stream);\n        result = stream.ToArray();\n    }\n\n    return result;\n}\n\npublic string SerializeToString<T>(T data)\n{\n    string result;\n    using (var stream = new MemoryStream())\n    {\n        this.SerializeToStream(data, stream);\n        result = Encoding.ASCII.GetString(stream.ToArray());\n    }\n\n    return result;\n}\n```\n\n在Format 成 String 我是採用ASCII的編碼方式，讓他出來會像16進位的感覺。\n\n> 也可以使用 Convert.ToBase64String(stream.ToArray()); 產生文字資料\n\n## ProtoBuf 反序列化\n\n在反序列化的部分也是預設採用 Stream 的資料格式來處理，\n\n### Simple Code 3 - 反序列化\n\n```cs\npublic T SerializeToStream<T>(Stream stream)\n{\n    return Serializer.Deserialize<T>(stream, data)\n}\n```\n\n### Simple Code 4 - 針對 Byte array and string deserialize\n\n```cs\npublic T DeSerializeFromBytes<T>(byte[] data)\n{\n    T result;\n    using (var stream = new MemoryStream(data))\n    {\n        stream.Seek(0, SeekOrigin.Begin);\n        result = this.DeSerializeFromStream<T>(stream);\n    }\n\n    return result;\n}\n\npublic T DeSerializeFromString<T>(string data)\n{\n    return this.DeSerializeFromBytes<T>(Encoding.ASCII.GetBytes(data));\n}\n```\n\n>注： 若string是使用base64字元輸出，那麼在deserialize時就必須使用\n>Convert.FromBase64String(data) 處理\n"},{"id":1475760616,"fileName":"protobuf-first-meet","url":"2016/10/06/protobuf-first-meet","title":"[ProtoBuf] 初次見面","description":"目前會採用ProtoBuf序列化是因為先前使用JSON有效能上的問題與在存取Redis上產生較高的延遲，所以改採用ProtoBuf來作為資料序列化與存放到Redis的主要格式。雖然在閱讀上需要自己寫小工具做轉譯，但在一般情況擁有較好的效能展現。","tags":["protobuf","serialize","deserialize"],"date":"2016-10-06T13:30:16.000Z","published":true,"content":"\n## 什麼是ProtoBuf？\n\n這是一個Google所開發出的資料儲存結構或物件（反）序列的結構（如 JOSN、XML、msgpack等），在官方網站的介紹上有這麼一段：\n> Protocol buffers are Google's language-neutral,\n> platform-neutral, extensible mechanism for serializing structured data\n> – think XML, but smaller, faster, and simpler.\n> You define how you want your data to be structured once,\n> then you can use special generated source code to easily write\n> and read your structured data to and from a variety of data streams and using a variety of languages.\n> <p style=\"text-align:right\"> from <a href=\"https://developers.google.com/protocol-buffers/\" target=\"_blank\">https://developers.google.com/protocol-buffers/</a></p>\n\nProtoBuf支援各種主流的語言(ex: C#, C++, JAVA等)，\n在官方中的[GitHub](https://github.com/google/protobuf)上有目前支援的語言實做，或是Third-party搜尋相關的實做套件。\n\n### ProtoBuf 優點\n\n* (反)序列**速度快**，方便於網路傳輸\n* 產出格式**內容小**，方便存放至檔案或其他Service\n\n### ProtoBuf 缺點\n\n* 二進位格式，難以閱讀\n* 使用上必須先作定義，需要先設定.proto檔案\n\n## 在 C# 中使用 ProtoBuf\n\n在專案中我是使用third-party的套件，[ProtoBuf-net](https://www.nuget.org/packages/protobuf-net)\n套件可以在nuget上找到，會選擇這個套件主要是因為在使用上較為簡便，也支援較多的設定的方式來做資料，設定的方式\n稍後會有比較詳細一些的介紹。\n\n### 定義方式\n\nProtoBuf-net設定的方式支援了以下三種方式\n\n#### Attribute\n\n這個方式，個人認為是一個較好的設定方式，\n\n```cs\n[ProtoContract]\npublic class TestClass\n{\n    [ProtoMember(1)]\n    public int TestA { get; set; }\n\n    [ProtoMember(2)]\n    public string TestB { get; set; }\n\n    [ProtoMember(3)]\n    public TestClassB TestC { get; set; }\n}\n\n[ProtoContract]\npublic class TestClassB\n{\n    ...\n}\n```\n\n這一個設定方式可以修改data model同時修改 Attribute 這樣在團隊使用上可以避免修改data model後\n也可馬上維護protobuf的設定，避免在runtime時因為設定上造成exception。\n\n#### .proto檔案\n\n這一個方式是官方的標準設定，這個需要額外產生一份.proto檔案，定義方式與格式可參考\n[官方網站](https://developers.google.com/protocol-buffers/docs/csharptutorial)\n的設定。\n\n這一個方式因為多產生了一個.proto的檔案，在修改data model後，必須要再額外修改.proto的檔案，\n有時候在開發上會有不一致的情況；在使用上與設定上也相對較為複雜。\n\n#### Runtime 定義\n\n這是在Application執行期間產生一份 RuntimeTypeModel，在Serializer的時候會以這一份RuntimeTypeModel為你的\ndata model做序列化，以下提供語法參考設定方式\n\n這一個方式會將原先定義覆蓋\n\n```cs\n    var metaType = RuntimeTypeModel.Default.Add(typeof(TestClassA), true);\n    metaType.AddSubType(100, typeof(TestA));\n\n    RuntimeTypeModel.Default.Add(typeof(TestA), false);\n```\n\n這一個會複寫(或新增)原先的定義\n```cs\n    var metaType = RuntimeTypeModel.Default;\n    var testA = metaType[typeof(TestClassA)];\n    testA.AddSubType(100, typeof(TestA));\n    metaType.Add(typeof(TestA), false);\n```\n\n## 總結\n\n目前會採用ProtoBuf序列化是因為先前使用JSON有效能上的問題與在存取Redis上產生較高的延遲，所以改採用\nProtoBuf來作為資料序列化與存放到Redis的主要格式。雖然在閱讀上需要自己寫小工具做轉譯，但在一般情況\n擁有較好的效能展現。\n\n\n其他更詳細的介紹可觀看\n[protobuf-net 官方GitHub](https://github.com/mgravell/protobuf-net#advanced-subjects)\n\n## 參考資料\n\n[Level up - protobuf-net - Serialize/DeSerialize data](http://larrynung.github.io/2016/08/23/protobuf-net-Serialize-DeSerialize-data/)\n\n[protobuf-net 官方GitHub](https://github.com/mgravell/protobuf-net)\n\n[Google protobuf](https://github.com/google/protobuf)\n"},{"id":1456314909,"fileName":"redis-pub-sub-application-notification","url":"2016/02/24/redis-pub-sub-application-notification","title":"【Redis】Redis Pub/Sub 製作應用程式間推播通知","description":"在很多時候我們的部屬環境是很複雜的，無法使用單一個應用程式來解決我們的問題...（例如：Windows server與Linux Server、多個網站或多個不同類型執行個體、網站搭配console應用程式）此時當應用程式間需要配合時是一個很重要的議題，很多人會把資料存放到資料庫，應用程式在定期去擷取尚未處理的資料，等相關資料處理完畢後再到資料庫注記哪些記錄是已經處理完畢的，在這樣的軟體設計與環境相對簡單許多，但是在系統繁忙的時間中，這可不是一個樂觀的狀態了..\n現在Redis具有這樣的功能，可以協助我們設計應用程式間可以互相配合的一個機制，如此一來我們就不需要在透過定期存取資料庫，相關的訊息或相關的資料可以及時的處理！","date":"2016-02-24T11:55:09.000Z","tags":["redis","pub","sub","notify"],"published":true,"content":"\n## 寫在前面\n\n在很多時候我們的部屬環境是很複雜的，無法使用單一個應用程式來解決我們的問題...（例如：Windows server與Linux Server、多個網站或多個不同類型執行個體、網站搭配console應用程式）此時當應用程式間需要配合時是一個很重要的議題，很多人會把資料存放到資料庫，應用程式在定期去擷取尚未處理的資料，等相關資料處理完畢後再到資料庫注記哪些記錄是已經處理完畢的，在這樣的軟體設計與環境相對簡單許多，但是在系統繁忙的時間中，這可不是一個樂觀的狀態了..\n\n現在Redis具有這樣的功能，可以協助我們設計應用程式間可以互相配合的一個機制，如此一來我們就不需要在透過定期存取資料庫，相關的訊息或相關的資料可以及時的處理！\n\n## Redis Command\n在使用推播前需要先瞭解三個指令，這三個指令是做應用程式推播時相關的指令\n\n### Publish\n\n#### 官網的說明\n>Available since 2.0.0.\n>\n>**Time complexity**: O(N+M) where N is the number of clients subscribed to the receiving channel and M is the total number of subscribed patterns (by any client).\n>\n>Postss a message to the given channel.\n\nRedis最低版本：2.0\n這個指令主要的用途就是將訊息推進某一個頻道中。\n而時間複雜度是：O(N+M)，N是訂閱這個頻道的Client數量，M是所有訂閱者的數量。\n\n#### Publish 指令的格式\n```shell\nPublish [channel] [message]\n```\n\n### Subscribe\n\n#### 官網的說明\n>Available since 2.0.0.\n>\n>**Time complexity**: O(N) where N is the number of channels to subscribe to.\n>\n>Subscribes the client to the specified channels.\n\nRedis最低版本：2.0\n這個指令主要的用途是訂閱頻道，用來接收訂閱頻道中的訊息。\n而時間複雜度是：O(N)，N是指要訂閱頻道的總數目\n\n#### Subscribe指令格式\n```shell\nSUBSCRIBE [channel 1] [channel 2] ...\n```\n\n### UnSubscribe\n\n#### 官網的說明\n>Available since 2.0.0.\n>\n>**Time complexity**: O(N) where N is the number of clients already subscribed to a channel.\n>\n>Unsubscribes the client from the given channels, or from all of them if none is given.\n\nRedis最低版本：2.0\n這個指令主要的用途是取消訂閱頻道。\n而時間複雜度是：O(N)，N是指要取消訂閱頻道的總數目\n\n#### UnSubscribe指令格式\n```shell\nUNSUBSCRIBE [channel 1] [channel 2] ...\n```\n\n## 系統開發\n\n### 存取Redis in C#：StackExchange.Redis\n這是一套存取Redis的一個套件，在稍候的程式開發中，都會使用到這個套件來存取Redis！\n\n```cs\nInstall-Package StackExchange.Redis\n```\n\n### Subscribe開發\n\n#### Step1. 連結Redis\n在使用Redis需要先建立connection，才能連結到Redis。再使用StackExchange.Redis 套件時需要先引入他的namespace。\n\n```cs\nusing StackExchange.Redis;\n```\n\n設定Redis連線可以參考[這篇文章](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Configuration.md#configuration-options)上面會有參數的說明，這邊沒有一一列舉相關的參數設定。\n\n```cs\nvar configurationOptions = new ConfigurationOptions\n{\n    AbortOnConnectFail = false,\n    Password = \"password\",\n    Ssl = false,\n    ConnectTimeout = 6000,\n    SyncTimeout = 6000\n};\nconfigurationOptions.EndPoints.Add(new DnsEndPoint(\"redis host\", port));\n// IP:\n// configurationOptions.EndPoints.Add(IP Address, port));\nConnectionMultiplexer redis = ConnectionMultiplexer.Connect(configurationOptions);\n```\n\n#### Step2. 訂閱頻道\n在StackExchange.Redis 取得連線後，開始開發訂閱頻道前要先取得Redis的Subcriber，才能發佈或是訂閱訊息\n\n```cs\nvar sub = redis.GetSubscriber();\n```\n\n在訂閱訊息前，先看一下StackExchange.Redis中Subscribe的說明\n第一個參數是頻道名稱，這邊可以直接使用字串來代表RedisChannel的型別，第二個是一個Action的委派型別，第三個是commandFlag，這個主要的用途是在描述指令該如何執行，這個參數可以省略。\n\n這裡做一個範例，我在這裡訂閱了一個Study4TW的頻道，然後指定了收到這個訊息要做哪些事情（寫在Do Something中）\n\n```cs\nsub.Subscribe(\"Study4TW\", (channel, message) =>\n {\n     // Do Something\n });\n```\n\n如此只要寫在系統loading時就可以訂閱到該頻道，這個只要做一次就可以了，不需要每次執行！\n\n#### Step3. 送出訊息！\n送出訊息的指令相當簡單，只要填上你是哪一個頻道，以及你要送出的訊息即可！在RedisChannel與RedisValue兩個型別都可以使用字串來傳遞即可。\n\n```cs\nvar sub = redis.GetSubscriber();\nsub.Publish(\"Study4TW\", message);\n```\n\n在搭配前面Step2的程式碼，在這樣簡單的範例就可以在應用程序間互相通訊了！就不再需要透過資料庫來做訊息通知了，也不用使用IF去判別這一個訊息要怎麼處理或是交給哪一個method處理，增加程式碼的可讀性！是否受用無窮阿？\n\n## 參考資料\n[Redis Pub/Sub](http://redis.io/topics/pubsub)\n\n[Publish - Redis](http://redis.io/commands/publish)\n\n[Subscribe - Redis](http://redis.io/commands/subscribe)\n\n[UnSubscribe - Redis](http://redis.io/commands/unsubscribe)\n\n[StackExchange.Redis GitHub](https://github.com/StackExchange/StackExchange.Redis)\n\n[StackExchange.Redis nuget](https://www.nuget.org/packages/StackExchange.Redis/)\n"}],"tags":{"postgresql":1,"database":2,".net":1,"aws":4,"devops":7,"prevision":6,"iot":2,"platformio":2,"arduino":2,"esp":1,"elk":3,"azure":4,"vulnerability":1,"ssl":2,"vmss":1,"cd":3,"study4":1,"dotnetconf":1,"selenium":2,"tdd":1,"react":3,"jest":1,"next.js":1,"frontend":1,"layout":1,"ec2":1,"iac":1,"terraform":1,"ci":1,"jenkins":1,"ecs":1,"ansible":1,"redis":2,"protobuf":2,"serialize":2,"deserialize":2,"pub":1,"sub":1,"notify":1}},"__N_SSG":true}